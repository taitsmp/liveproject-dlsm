{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "This time we can use a larger chunk of the original dataset without running into so many memory and runtime problems.\n",
    "\n",
    "1. Explore and analyze the set of unique characters present in the dataset.\n",
    "1. Subsample the dataset to take into account the titles only.\n",
    "1. Implement the tokenization of the dataset and transform the tokens into AllenNLP Instances.\n",
    "1. Design a RNN with AllenNLP that includes:\n",
    "   1. an embedding of the tokens,\n",
    "   1. a seq2seq LSTM layer,\n",
    "   1. a feed forward layer that outputs probability distribution of the characters\n",
    "1. Train the model.\n",
    "1. Evaluate the model by calculating the loss of some sentences and by generating text.\n",
    "\n",
    "### Bonus\n",
    "\n",
    "Can you swap a Transformer-based model in for an LSTM? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* [AllenNLP the hard way: Building a Baseline Model](https://jbarrow.ai/allennlp-the-hard-way-2/)\n",
    "* [Sequential Labeling and Language Modeling (Chapter 5 - Covers allenNLP and Char RNN LM)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-5/v-5/35)\n",
    "* [Atentions and the Transformer (Chapter 8)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-8/v-5/)\n",
    "* [Sequence to Sequence Models (Chapter 6)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-6/v-5/)\n",
    "* [An In Depth AllenNLP tutorial (Basics to BERT)](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)\n",
    "* [Official AllenNLP tutorial](https://allennlp.org/tutorials)\n",
    "* [AllenNLP Guide - Reading Data](https://guide.allennlp.org/reading-data)\n",
    "* [AllenNLP Simple Language Model DatasetReader](https://github.com/allenai/allennlp-models/blob/master/allennlp_models/lm/dataset_readers/simple_language_modeling.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "\n",
    "from torch.nn import LSTM\n",
    "\n",
    "from typing import Dict, Iterable, Union, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK_FILE= 'stackexchange_tokenized.csv'\n",
    "df = pd.read_csv(f'data/{TOK_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "      <td>29</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "      <td>18</td>\n",
       "      <td>What is normality ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "      <td>65</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "      <td>58</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "      <td>50</td>\n",
       "      <td>The Two Cultures : statistics vs. machine lear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  post_id  parent_id  comment_id  \\\n",
       "0           0        1        NaN         NaN   \n",
       "1           1        2        NaN         NaN   \n",
       "2           2        3        NaN         NaN   \n",
       "3           3        4        NaN         NaN   \n",
       "4           4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  length  \\\n",
       "0                      Eliciting priors from experts    title      29   \n",
       "1                                 What is normality?    title      18   \n",
       "2  What are some valuable Statistical Analysis op...    title      65   \n",
       "3  Assessing the significance of differences in d...    title      58   \n",
       "4  The Two Cultures: statistics vs. machine learn...    title      50   \n",
       "\n",
       "                                           tokenized  \n",
       "0                      Eliciting priors from experts  \n",
       "1                                What is normality ?  \n",
       "2  What are some valuable Statistical Analysis op...  \n",
       "3  Assessing the significance of differences in d...  \n",
       "4  The Two Cultures : statistics vs. machine lear...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit Dataset to Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles = df[df.category == 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91648"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "809166"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCtr = Counter()\n",
    "\n",
    "def collect_chars(txt):\n",
    "    global charCtr   \n",
    "    charCtr.update(list(txt))\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 1456196),\n",
       " ('e', 981638),\n",
       " ('i', 817482),\n",
       " ('t', 789104),\n",
       " ('a', 737638),\n",
       " ('o', 714848),\n",
       " ('n', 684648),\n",
       " ('r', 600202),\n",
       " ('s', 593196),\n",
       " ('l', 365504),\n",
       " ('d', 298382),\n",
       " ('c', 279640),\n",
       " ('m', 246208),\n",
       " ('u', 224208),\n",
       " ('f', 221880)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titles['text'].apply(collect_chars)\n",
    "charCtr.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nStrategy\\n\\n* work on text_to_instance first. \\n* when you implement read you could probably pull it from the dataframe.  In the real world this might be slow.\\n\\nQuestions\\n\\n* What are token indexers?\\n* Do I need both input and output tokens captured in my data reader?  I assume I can make the call. See example below.\\n* \\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#@DatasetReader.register(\"character_based_lm_so\")\n",
    "class CharDatasetReader(DatasetReader):\n",
    "    def __init__() -> None:\n",
    "        \n",
    "        #todo: could become args\n",
    "        self._token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
    "        self._tokenizer = CharacterTokenizer()\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def text_to_instance(self, sentence: str,) -> Instance:\n",
    "        \n",
    "        tokenized = self._tokenizer.tokenize(sentence)\n",
    "        #TODO: add start and end characters\n",
    "        #TODO: do you want to add \"source\" and \"target\" here? \n",
    "        instance = Instance({\"source\": TextField(tokenized, self._token_indexers)})\n",
    "        return instance\n",
    "    \n",
    "    def _read(df: pd.DataFrame) -> Iterable[Instance]:\n",
    "        \n",
    "        for row in df.itertuples(index=False):\n",
    "            instance = self.text_to_instance(row.text)\n",
    "            yield instance\n",
    "\n",
    "#left off here -https://jbarrow.ai/allennlp-the-hard-way-1/ (Good info on testing your datasetReader)\n",
    "\n",
    "'''\n",
    "\n",
    "Strategy\n",
    "\n",
    "* work on text_to_instance first. \n",
    "* when you implement read you could probably pull it from the dataframe.  In the real world this might be slow.\n",
    "\n",
    "Questions\n",
    "\n",
    "* What are token indexers?\n",
    "* Do I need both input and output tokens captured in my data reader?  I assume I can make the call. See example below.\n",
    "* \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register('char_lstm')\n",
    "class CharLSTM(Model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PytorchSeq2VecWrapper(\n",
    "     LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CharacterTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ct.tokenize('this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start and end symbols you should use when training your language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "\n",
    "\n",
    "tokens.insert(0, Token(START_SYMBOL))\n",
    "tokens.append(Token(END_SYMBOL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    " \n",
    "token_indexers = {'tokens': SingleIdTokenIndexer()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the beginning of how you'd create a DatasetReader.  Implement `_read` function to return an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.instance import Instance\n",
    " \n",
    "input_field = TextField(tokens[:-1], token_indexers)\n",
    "output_field = TextField(tokens[1:], token_indexers)\n",
    "instance = Instance({'input_tokens': input_field,\n",
    "                     'output_tokens': output_field})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliciting priors from experts\n"
     ]
    }
   ],
   "source": [
    "for row in df.itertuples(index=False):\n",
    "    print(row.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allenNLP",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
