{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a simple Sequential Model from an LSTM. \n",
    "\n",
    "* Given a sequence of n-grams, the classifier predicts the following token as a class. \n",
    "* The input of the neural network is an array of sequences of tokens for the design matrix and the output a vector of labels that corresponds to the target token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "In the case of the n-gram language model, we used the probability of each n-gram in the input sentence to calculate the perplexity. Our current model does not rely on n-grams, but on probabilities of sequences of tokens to be followed by a subsequent token. We need to adapt the perplexity formula for n-grams language models to sequence-based language models.\n",
    "\n",
    "If we consider the sentence of N tokens:\n",
    "\n",
    "$$w_{1},\\cdots, w_N$$\n",
    "\n",
    "Then we can calculate the probability of that sentence as the product of probabilities of all the padded subsequences. Let’s take an example of a 3-token sentence.\n",
    "\n",
    "$$ P(w_1,w_2, w_3) =  P(w_3 | w_1, w_2) \\times p(w_2 | w_1 ,0)  \\times p(w_1 | 0 ,0) $$\n",
    "\n",
    "In general, for a sentence of N tokens and a sequence length of length S,\n",
    "\n",
    "$$ P(w_1,\\cdots, w_N) = \\prod_{k = 1}^{ \\max{(N,S)}} P(w_k | \\text{padded}_S(w_{1}, \\cdots, w_{k-1})    )  $$\n",
    "\n",
    "where $ P(w_{k} | \\text{padded}_S(w_{1}, \\cdots, w_{k-1}) $ is precisely the probability given by the classification model.\n",
    "\n",
    "We can therefore compute the perplexity of a sentence of length N with\n",
    "\n",
    "$$PP(w_{1},\\cdots, w_N) = \\exp [ - \\frac{1}{N} {\\sum_{i = 1}^{ \\max{(N,S)} } \\log { P(w_{k} | \\text{padded}_S(w_{1}, \\cdots, w_{k-1}) } } ) ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "#### Preparing the data\n",
    "\n",
    "1. Load the dataset that was prepared in task 1.\n",
    "2. The original dataset is too large and needs to be reduced. To reduce it, you can, for instance,\n",
    "   + filter out items that have too many or too little tokens,\n",
    "   + select items of a certain type: post, comments, or titles, or\n",
    "   + or sub sample items randomly.\n",
    "3. Build the vocabulary as the set of all unique tokens to construct the list of token indexes.\n",
    "  + Filtering on token frequency is one way to reduce the overall size of the vocabulary.\n",
    "4. Set a fixed sequence length and build sequences of token indexes from the corpus. (See for instance keras pad_sequences.)\n",
    "5. Split the sequences into predictors and labels (`keras.utils.to_categorical`)\n",
    "\n",
    "#### The model\n",
    "\n",
    "The data is now ready to be used to fit a neural network.\n",
    "\n",
    "1. Define a simple sequential model with an embedding layer, LSTM(s), and a dense layer with softmax activation. Feel free to experiment with dropouts, different optimizers. You can use any type of neural net you want; for example, Keras, TensorFlow, PyTorch, and so on.\n",
    "2. Specify the number of epochs, the batch size, and other fitting parameters.\n",
    "3. Fit the network.\n",
    "\n",
    "#### Assessing the results\n",
    "\n",
    "1. Write a function that generates text.\n",
    "2. Generate some text and take note of:\n",
    "  - Token repetitions\n",
    "  - Missing punctuations\n",
    "  - Other anomalies\n",
    "3. Write a function that calculates **perplexity of a sentence** and apply it to a subset of sentences to evaluate the model.\n",
    "4. Define a validation set; for instance, 1000 titles.\n",
    "5. Transform that validation set into sequences of tokens using the training vocabulary.\n",
    "6. Tune the neural net and the parameters of the preprocessing phase to improve the model’s perplexity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK_FILE= 'stackexchange_tokenized.csv'\n",
    "df = pd.read_csv(f'data/{TOK_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabularly from the set of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the right fixed sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the sequence into predictors and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rnn_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "### views in pytorch - https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "### Maybe skip this one - https://gist.github.com/williamFalcon/f27c7b90e34b4ba88ced042d9ef33edd\n",
    "### https://pytorch.org/docs/master/generated/torch.nn.LSTM.html\n",
    "### https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "### https://stackoverflow.com/a/42482819 reshaping tensors with \"view\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, seq_len, vocab_size, embedding_dim, hidden_dim): #TODO: DO I NEED THE seq_len here? Doubt it. \n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm =  nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # LEFT OFF HERE - decide how/when to reset and share hidden params? Consider bptt approach. \n",
    "        # when you process inputs, will batches be contiguous? Are there any helper function in pytorch you can use?\n",
    "        \n",
    "        seq_len, batch_size = x.shape   \n",
    "        x = self.word_embeddings(x)\n",
    "        x, states = self.lstm(x) # -> (seq_len, batch_size, hidden_size)\n",
    "        x = self.dense(x)        # -> (seq_len, batch_size, vocab_size)\n",
    "        x = F.log_softmax(x, dim=2) \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of epochs, the batch size, and other fitting parameters\n",
    "\n",
    "# TODO: these numbers are all placeholders for testing. \n",
    "seq_len = 50\n",
    "vocab_size = 4000\n",
    "embedding_dim = 200\n",
    "hidden_dim = 40\n",
    "batch_size = 15\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(seq_len, vocab_size, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 15])\n",
      "torch.Size([50, 15, 4000])\n"
     ]
    }
   ],
   "source": [
    "# run `forward` here to make sure that you have the expected dimensions for inputs and outputs.\n",
    "\n",
    "#fake input\n",
    "tsr = torch.randint(0,vocab_size, (seq_len, batch_size))\n",
    "print(tsr.shape)  \n",
    "\n",
    "out_tensor = net(tsr)\n",
    "print(out_tensor.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function that generates text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the function for errors (missing text, punctuation, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a validation set; for instance, 1000 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform that validation set into sequences of tokens using the training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the neural net and the parameters of the preprocessing phase to improve the model’s perplexity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 30])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "emb = nn.Embedding(400, 30)\n",
    "tsr = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "ts2 = emb(tsr)\n",
    "ts2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6, 9, 7],\n",
       "        [3, 6, 8, 7],\n",
       "        [0, 8, 9, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how to get random tensors that are integers\n",
    "torch.randint(0,10, (3,4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied code from https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "# testing model dimensions. \n",
    "rnn_lm = RNNModel('LSTM', vocab_size, seq_len, embedding_dim, 1, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([750, 4000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = rnn_lm.init_hidden(batch_size)\n",
    "out_tensor, _ = rnn_lm(tsr, hidden)\n",
    "out_tensor.shape #same output size as my model. **update** they do this intentionally for easy loss calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 15, 4000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out = torch.randint(0,vocab_size, (seq_len, batch_size, vocab_size)).float()\n",
    "#rszd_out = lstm_out.view(seq_len, batch_size, -1) #this is a noop. \n",
    "rszd_out = F.softmax(rszd_out, dim=2) #removed log\n",
    "rszd_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "summed = torch.sum(rszd_out, dim=2)\n",
    "summed[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2152,  453, 2541, 3116],\n",
       "         [1831, 3638, 2561, 1046],\n",
       "         [1450, 2765, 3805, 1294]],\n",
       "\n",
       "        [[3281, 2037,  266, 3607],\n",
       "         [3907, 2355, 3675, 1584],\n",
       "         [3511, 2891, 3937, 2514]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0,vocab_size, (2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
