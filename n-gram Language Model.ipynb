{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "1. **DONE** - Split the dataset into a training and a testing subset. Use the category “title” for the testing set and the categories “comment” and “post” for the training set. The short length of titles will make them good candidates later as seeds for text generation.\n",
    "1. **DONE** - Build the matrix of prefix—word frequencies.\n",
    "  + Use the `ngrams` function from `nltk.utils` to generate all n-grams from the corpus\n",
    "  + Set the following `left_pad_symbol = <s> and right_pad_symbol = </s>`\n",
    "1. **DONE** - Write a text generation function:\n",
    "  + takes a bigram as input and generates the next token\n",
    "  + iteratively slide the prefix over the generated text so that the new prefix includes the most recent token; generates the next token\n",
    "  + to generate each next token, sample the list of words associated with the prefix using the probability distribution of the prefix\n",
    "  + stop the text generation when a certain number of words have been generated or the latest token is a `</s>`.\n",
    "1. **DONE** - Write a function that can estimate the probability of a sentence and use it to select the most probable sentence out of several candidate sentences.\n",
    " + Split the sentence into trigrams and use the chain rule to calculate the probability of the sentence as a product of the bigrams—tokens probabilities\n",
    "1. **IN PROGRESS** - Implement the **perplexity** scoring function for a given sentence and for the training corpus.\n",
    "  + The perplexity of a corpus is obtained by multiplying the perplexity of each sentence in the corpus.\n",
    "1. **DONE** - Implement **Additive Laplace** smoothing to give a non-zero probability to missing prefix—token combinations when calculating perplexity.\n",
    "1. Calculate the perplexity of the language model on the test set composed of titles.\n",
    "1. Try to improve the perplexity score of your model by:\n",
    "  + modifying the preprocessing phase of the corpus,\n",
    "  + increasing or decreasing number of tokens in the model (bi grams, 4-grams, etc.),\n",
    "  + varying the delta parameter in the Additive Laplace smoothing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace Smoothing \n",
    "\n",
    "$$ p(w_{1} w_2 w_3) = p(w_3 / w_{1} w_{2}) = \\frac{count(w_{1}w_{2} w_3)+ \\delta} {count(w_{1}w_{2}) + \\delta \\times |N|}$$\n",
    "\n",
    "$$p(token / prefix) = \\frac{count( prefix + token) + \\delta} {count(prefix) + \\delta \\times | N |}$$\n",
    "\n",
    "##### n-gram is missing from the corpus\n",
    "\n",
    "$$p(token / prefix) = \\frac{ \\delta} {count(prefix) + \\delta \\times | N |}$$\n",
    "\n",
    "##### prefix is missing from corpus\n",
    "\n",
    "$$p(token / prefix) = \\frac{ \\delta} { \\delta \\times | N |}$$\n",
    "\n",
    "* $ | N | $ is the size of size of the vocabulary\n",
    "* $ \\delta $ is a constant that's less than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* https://programminghistorian.org/en/lessons/counting-frequencies\n",
    "* https://www.kite.com/python/docs/nltk.ngrams\n",
    "* https://stackoverflow.com/questions/54941966/how-can-i-calculate-perplexity-using-nltk (using off the shelf NLTK for perplexity and MLE bigram estimates, etc.)\n",
    "* https://stats.stackexchange.com/questions/129352/how-to-find-the-perplexity-of-a-corpus \n",
    "* https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "TOK_FILE= 'stackexchange_tokenized.csv'\n",
    "df = pd.read_csv(f'data/{TOK_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['title', 'post', 'comment'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = df['category'] == 'title'\n",
    "posts = df['category'] == 'post'\n",
    "commt = df['category'] == 'comment'\n",
    "\n",
    "test_df  = df[title]\n",
    "train_df = df[posts | commt] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    91648\n",
       "post_id       91648\n",
       "parent_id         0\n",
       "comment_id        0\n",
       "text          91648\n",
       "category      91648\n",
       "length        91648\n",
       "tokenized     91648\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    717518\n",
       "post_id       717518\n",
       "parent_id      75430\n",
       "comment_id    550329\n",
       "text          717518\n",
       "category      717518\n",
       "length        717518\n",
       "tokenized     717518\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the matrix of prefix—word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_counts = defaultdict(Counter) \n",
    "freq = dict()\n",
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counts(s_string):\n",
    "    global vocab, raw_counts\n",
    "    s_list = s_string.split(' ')\n",
    "    vocab.update({*s_list})\n",
    "    \n",
    "    for tg in ngrams(s_list, 3, pad_left=True, pad_right=True, right_pad_symbol='</s>', left_pad_symbol='<s>'):\n",
    "        bg = tg[:-1]\n",
    "        wd = tg[-1]\n",
    "        if not bg in raw_counts:\n",
    "            raw_counts[bg] = Counter()\n",
    "        raw_counts[bg][wd] += 1\n",
    "\n",
    "\n",
    "def counts_to_freq():\n",
    "    for bg, cntr in raw_counts.items():\n",
    "        tot = sum(cntr.values())\n",
    "        freq[bg] = { i:cntr[i]/tot for i in cntr }\n",
    "        \n",
    "def sample_next(bg):\n",
    "    nwd = freq[tuple(bg)]\n",
    "    nw  = rng.choice(list(nwd.keys()), 1, p=list(nwd.values()))[0]\n",
    "    return nw\n",
    "\n",
    "#pass wc=0 if you just want to generate one sentence\n",
    "def generate_text(seed, wc=0):\n",
    "    \n",
    "    if type(seed) == str:\n",
    "        seed = ['<s>', 'I'] + seed.split(' ') #won't matter if there are extra things on the list\n",
    "    bg = seed[-2:]\n",
    "    \n",
    "    gt = bg #this could be cleaned up\n",
    "    \n",
    "    one_sentence = False\n",
    "    if wc==0:\n",
    "        one_sentence = True\n",
    "        wc = 50\n",
    "    \n",
    "    while wc > 0:\n",
    "        nw = sample_next(bg)\n",
    "        gt.append(nw)\n",
    "        wc -=1\n",
    "        if one_sentence and gt[-1] == '</s>':\n",
    "            break\n",
    "        bg = gt[-2:]\n",
    "    return gt\n",
    "\n",
    "\n",
    "def prob_of_sent(s):\n",
    "    probs = get_probs(s)\n",
    "    \n",
    "    lprobs = [math.log(p) for p in probs]\n",
    "    return math.exp(sum(lprobs))\n",
    "\n",
    "def lps_prob_of_sent(s):\n",
    "    probs = laplace_smoothed_prob(s)\n",
    "    \n",
    "    lprobs = [math.log(p) for p in probs]\n",
    "    return math.exp(sum(lprobs))\n",
    "\n",
    "\n",
    "def laplace_smoothed_prob(s):\n",
    "    global vocab\n",
    "    probs = []\n",
    "    vocab_size = len(vocab)\n",
    "    delta  = 0.25  \n",
    "\n",
    "    if type(s) == str:\n",
    "        s = s.split(' ') #small error. Watch out passing in sentence with punctuation not space-separated from the last word.  \n",
    "    \n",
    "    tgrams = ngrams(s, 3,  pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "    #LEFT OFF HERE. NEED TO TEST THIS.\n",
    "    for tg in tgrams:\n",
    "        bg = tg[:-1]\n",
    "        wd = tg[-1] \n",
    "        tgc = delta\n",
    "        bgc = delta * vocab_size\n",
    "        if bg in raw_counts:\n",
    "            bgc += sum(raw_counts[bg].values())\n",
    "            tgc += raw_counts[bg][wd]\n",
    "        probs.append(tgc/bgc)\n",
    "    return probs\n",
    "                \n",
    "\n",
    "def get_probs(s):\n",
    "    if type(s) == str:\n",
    "        s = s.split(' ') #small error. Watch out passing in sentence with punctuation not space-separated from the last word.  \n",
    "    \n",
    "    mprob  = 0.005 #TODO: what is the right value?\n",
    "    probs  = []\n",
    "    tgrams = ngrams(s, 3,  pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "    for tg in tgrams:\n",
    "        bg = tg[:-1]\n",
    "        wd = tg[-1]\n",
    "        if not bg in freq:\n",
    "            probs.append(mprob)\n",
    "        else:\n",
    "            pb = freq[bg][wd] if wd in freq[bg] else mprob\n",
    "            probs.append(pb)\n",
    "    return probs\n",
    "\n",
    "def print_it(str):\n",
    "    print(str+\"\\n\")\n",
    "\n",
    "def perplexity_old(s):\n",
    "    #steps\n",
    "    # * get probs for all trigrams.\n",
    "    # * calc 1/p for each\n",
    "    # * take the log of each \n",
    "    # * sum up the logs  \n",
    "    # * multiple the result by 1/(len(s)) \n",
    "    # * exp() the result. \n",
    "    lip = [math.log(1/p) for p in get_probs(s)]  \n",
    "    N = len(lip)\n",
    "    return math.exp((1/N) * sum(lip))\n",
    "\n",
    "#second attempt at perplexity. Should yield the same result. \n",
    "def perplexity(s, nrm=None):\n",
    "    \n",
    "    # * get probs for all trigrams.\n",
    "    # * take the log of each \n",
    "    # * sum up the logs  \n",
    "    # * multiple the result by - 1/(len(s)) (NOTE change in sign)\n",
    "    # * exp() the result.     \n",
    "    lp = [math.log(p) for p in get_probs(s)]\n",
    "    N  = nrm if nrm else len(lp)\n",
    "    return math.exp((-1/N) * sum(lp))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 1min 49s, sys: 903 ms, total: 1min 49s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df['tokenized'].apply(bigram_counts)\n",
    "print('done')\n",
    "\n",
    "#train_df['tokenized'].head(n=150).apply(bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 8.1 s, sys: 480 ms, total: 8.58 s\n",
      "Wall time: 8.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counts_to_freq()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT OFF HERE - Overflow.  \n",
    "#corpus perplexity\n",
    "c_perp = 0\n",
    "def corpus_perplexity_step1(s):\n",
    "    global c_perp\n",
    "    c_perp = c_perp + math.log(perplexity(s))\n",
    "\n",
    "    \n",
    "# maybe helpful with overflow error \n",
    "# https://stats.stackexchange.com/questions/129352/how-to-find-the-perplexity-of-a-corpus\n",
    "df['tokenized'].apply(corpus_perplexity_step1)\n",
    "c_perp = math.exp(c_perp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK implementation of Perplexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from - https://stackoverflow.com/a/55043954\n",
    "\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary\n",
    "\n",
    "padded_vocab = Vocabulary(list(vocab)) #need to add padding characters here? \n",
    "tokenized_text =train_df['tokenized'].to_list()\n",
    "\n",
    "n = 2\n",
    "train_data = [ngrams(t, 2,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_vocab)\n",
    "\n",
    "# LEFT OFF HERE - do I care to do more?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.09986104842617\n",
      "10.288409386697566\n"
     ]
    }
   ],
   "source": [
    "def to_bigram(sent):\n",
    "    return ngrams(sent, 2,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\")\n",
    "\n",
    "\n",
    "#not that different.\n",
    "print(model.perplexity(to_bigram(\"Thank you\")))\n",
    "print(model.perplexity(to_bigram(\"Thank you for the butterfly .\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.60285946983705e-08\n",
      "6.631070049074365e-12\n",
      "3.8911442646285767e-23\n",
      "1.484185689441057e-35\n"
     ]
    }
   ],
   "source": [
    "p = lps_prob_of_sent(\"Thank you\") \n",
    "print(p)\n",
    "p = lps_prob_of_sent(\"Thank you for the answer .\")\n",
    "print(p)\n",
    "p = lps_prob_of_sent(\"Thank you for the butterfly .\")\n",
    "print(p)\n",
    "p = lps_prob_of_sent(\"zzz xx ff afe\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.914399620199843\n",
      "10.12746589825184\n"
     ]
    }
   ],
   "source": [
    "p = perplexity(\"Thank you\")\n",
    "print(p)\n",
    "p = perplexity(\"Thank you for the butterfly .\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.914399620199843\n",
      "10.12746589825184\n"
     ]
    }
   ],
   "source": [
    "p = perplexity_old(\"Thank you\")\n",
    "print(p)\n",
    "p = perplexity_old(\"Thank you for the butterfly .\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9148676411688643"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after calculating this by hand it's correct. \n",
    "lip = [math.log(1/p) for p in [1/2, 1/3, 1/10]]\n",
    "N = len(lip)\n",
    "math.exp((1/N) * sum(lip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', 'Thank'),\n",
       " ('<s>', 'Thank', 'you'),\n",
       " ('Thank', 'you', '</s>'),\n",
       " ('you', '</s>', '</s>')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(\"Thank you\".split(\" \"), 3,  pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12393\n",
      "120395\n"
     ]
    }
   ],
   "source": [
    "start_counter = raw_counts[('<s>', '<s>')]\n",
    "print(start_counter['Thank'])\n",
    "print(start_counter['I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004375040679188112\n",
      "1.21292662159124e-06\n",
      "9.036363322889784e-09\n",
      "1.5625000000000006e-14\n",
      "5.6052210670627125e-15\n"
     ]
    }
   ],
   "source": [
    "p = prob_of_sent(\"Thank you\") #surprised this isn't a higher prob. \n",
    "print(p)\n",
    "p = prob_of_sent(\"Thank you for the answer .\")\n",
    "print(p)\n",
    "p = prob_of_sent(\"Thank you for the butterfly .\")\n",
    "print(p)\n",
    "p = prob_of_sent(\"zzz xx ff afe\")\n",
    "print(p)\n",
    "p = prob_of_sent(\"zzz xx ff afe you .\")\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.', '.', 'you', '.'], dtype='<U3')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.choice([',', 'you', 'no', '.'], 4, p=[0.2, 0.2, 0.2, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.choice([',', 'you', 'no', '.'], 1, p=[0.2, 0.2, 0.2, 0.4])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 0.2, 'you': 0.2, 'no': 0.2, '.': 0.4}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq[tuple(['Hello', 'there'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'there',\n",
       " '.',\n",
       " 'The',\n",
       " 'unbiased',\n",
       " 'maximum',\n",
       " 'likelihood',\n",
       " 'to',\n",
       " 'produce',\n",
       " 'simulated',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'way',\n",
       " 'things',\n",
       " 'are',\n",
       " 'not',\n",
       " 'clear',\n",
       " 'which',\n",
       " 'models',\n",
       " 'did',\n",
       " '?',\n",
       " '</s>']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(freq.keys())[:10]\n",
    "type(keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How', 'should'),\n",
       " ('should', 'I'),\n",
       " ('I', 'elicit'),\n",
       " ('elicit', 'prior'),\n",
       " ('prior', 'distributions'),\n",
       " ('distributions', 'from'),\n",
       " ('from', 'experts'),\n",
       " ('experts', 'when'),\n",
       " ('when', 'fitting'),\n",
       " ('fitting', 'a')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(freq.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'I': 0.7388748950461796,\n",
       "  'economists': 0.0008396305625524769,\n",
       "  'you': 0.012594458438287154,\n",
       "  'one': 0.04953820319059614,\n",
       "  'we': 0.033585222502099076,\n",
       "  'such': 0.0041981528127623844,\n",
       "  'meta': 0.0008396305625524769,\n",
       "  'these': 0.007556675062972292,\n",
       "  'it': 0.014273719563392108,\n",
       "  'repeated': 0.0008396305625524769,\n",
       "  'the': 0.036943744752308987,\n",
       "  '/': 0.0025188916876574307,\n",
       "  'decision': 0.0016792611251049538,\n",
       "  'choose': 0.0008396305625524769,\n",
       "  'i': 0.0218303946263644,\n",
       "  'that': 0.005877413937867338,\n",
       "  'she': 0.0008396305625524769,\n",
       "  'a': 0.0033585222502099076,\n",
       "  'criterion': 0.0008396305625524769,\n",
       "  'this': 0.020990764063811923,\n",
       "  'my': 0.0025188916876574307,\n",
       "  \"'\": 0.0008396305625524769,\n",
       "  'proceed': 0.0008396305625524769,\n",
       "  'be': 0.0041981528127623844,\n",
       "  'x4': 0.0008396305625524769,\n",
       "  'tiny': 0.0025188916876574307,\n",
       "  'approach': 0.0016792611251049538,\n",
       "  'outliers': 0.005037783375314861,\n",
       "  ',': 0.0008396305625524769,\n",
       "  'they': 0.0025188916876574307,\n",
       "  'data': 0.0008396305625524769,\n",
       "  'accuracy': 0.0008396305625524769,\n",
       "  'LSTM': 0.0008396305625524769,\n",
       "  '(': 0.0008396305625524769,\n",
       "  'do': 0.0008396305625524769,\n",
       "  'he': 0.0016792611251049538,\n",
       "  'people': 0.0008396305625524769,\n",
       "  'regression': 0.0008396305625524769,\n",
       "  'interpret': 0.0008396305625524769,\n",
       "  'change': 0.0008396305625524769,\n",
       "  'Bonferroni': 0.0008396305625524769,\n",
       "  'our': 0.0008396305625524769,\n",
       "  'an': 0.0008396305625524769,\n",
       "  'explain': 0.0008396305625524769,\n",
       "  'then': 0.0008396305625524769,\n",
       "  'imputation': 0.0008396305625524769,\n",
       "  'someone': 0.0008396305625524769,\n",
       "  'AIC': 0.0008396305625524769,\n",
       "  'Type': 0.0008396305625524769,\n",
       "  'mixed': 0.0008396305625524769,\n",
       "  '\"': 0.0008396305625524769,\n",
       "  'duplicate': 0.0008396305625524769},\n",
       " {'elicit': 0.0001322226629644321,\n",
       "  'use': 0.192780642602142,\n",
       "  'model': 0.002115562607430914,\n",
       "  'install': 0.0001322226629644321,\n",
       "  'search': 0.001190003966679889,\n",
       "  'obtain': 0.0006611133148221605,\n",
       "  'be': 0.04535237339680021,\n",
       "  'take': 0.016527832870554014,\n",
       "  'then': 0.004760015866719556,\n",
       "  'do': 0.11648816607166468,\n",
       "  'transform': 0.005685574507470581,\n",
       "  'employ': 0.0006611133148221605,\n",
       "  'inflate': 0.0002644453259288642,\n",
       "  'plot': 0.001190003966679889,\n",
       "  'mathematically': 0.0001322226629644321,\n",
       "  'display': 0.0002644453259288642,\n",
       "  'spend': 0.0003966679888932963,\n",
       "  'worry': 0.0017188946185376173,\n",
       "  'find': 0.00251223059632421,\n",
       "  'cut': 0.0001322226629644321,\n",
       "  'choose': 0.019965622107629248,\n",
       "  'think': 0.0031733439111463705,\n",
       "  'calculate': 0.009652254396403544,\n",
       "  'scan': 0.0001322226629644321,\n",
       "  'pick': 0.002776675922253074,\n",
       "  'set': 0.003702234563004099,\n",
       "  'analyze': 0.002644453259288642,\n",
       "  'look': 0.014015602274229802,\n",
       "  'continue': 0.0015866719555731853,\n",
       "  'multiply': 0.0006611133148221605,\n",
       "  'try': 0.010974481026047865,\n",
       "  'treat': 0.007007801137114901,\n",
       "  'make': 0.005553351844506148,\n",
       "  'apply': 0.009916699722332408,\n",
       "  'subtract': 0.0003966679888932963,\n",
       "  'analyse': 0.001190003966679889,\n",
       "  'say': 0.005421129181541716,\n",
       "  'delete': 0.0030411212481819385,\n",
       "  'care': 0.0014544492926087532,\n",
       "  'run': 0.00872669575565252,\n",
       "  'define': 0.002644453259288642,\n",
       "  'solve': 0.0007933359777865926,\n",
       "  'approach': 0.008991141081581383,\n",
       "  'always': 0.002380007933359778,\n",
       "  'account': 0.0005288906518577285,\n",
       "  'assign': 0.0015866719555731853,\n",
       "  'start': 0.005288906518577284,\n",
       "  'give': 0.002776675922253074,\n",
       "  'stick': 0.002380007933359778,\n",
       "  'create': 0.002644453259288642,\n",
       "  'only': 0.003834457225968531,\n",
       "  'add': 0.00436334787782626,\n",
       "  'determine': 0.003834457225968531,\n",
       "  'build': 0.00251223059632421,\n",
       "  'test': 0.005950019833399445,\n",
       "  'just': 0.022477852703953458,\n",
       "  'construct': 0.0009255586407510248,\n",
       "  'control': 0.0006611133148221605,\n",
       "  'submit': 0.0002644453259288642,\n",
       "  'read': 0.0035700119000396666,\n",
       "  'go': 0.02009784477059368,\n",
       "  'interpret': 0.03067565780774825,\n",
       "  'report': 0.006346687822292741,\n",
       "  'consider': 0.016263387544625148,\n",
       "  'not': 0.005685574507470581,\n",
       "  'describe': 0.001057781303715457,\n",
       "  'deal': 0.006346687822292741,\n",
       "  'attribute': 0.0001322226629644321,\n",
       "  'rather': 0.002380007933359778,\n",
       "  'include': 0.008065582440830358,\n",
       "  'study': 0.001322226629644321,\n",
       "  'decide': 0.002776675922253074,\n",
       "  'maybe': 0.0005288906518577285,\n",
       "  'sort': 0.0002644453259288642,\n",
       "  'discretize': 0.0001322226629644321,\n",
       "  'fit': 0.002115562607430914,\n",
       "  'adjust': 0.0017188946185376173,\n",
       "  'first': 0.0034377892370752346,\n",
       "  'proceed': 0.014147824937194235,\n",
       "  'have': 0.007933359777865925,\n",
       "  'design': 0.0015866719555731853,\n",
       "  'check': 0.003966679888932963,\n",
       "  'chose': 0.0005288906518577285,\n",
       "  'conduct': 0.002247785270395346,\n",
       "  'perform': 0.00753669178897263,\n",
       "  'understand': 0.002776675922253074,\n",
       "  'compare': 0.005156683855612852,\n",
       "  'correct': 0.0019833399444664813,\n",
       "  'measure': 0.002644453259288642,\n",
       "  'select': 0.0030411212481819385,\n",
       "  'handle': 0.004627793203755124,\n",
       "  'combine': 0.0019833399444664813,\n",
       "  'propagate': 0.0001322226629644321,\n",
       "  'focus': 0.0007933359777865926,\n",
       "  'answer': 0.0009255586407510248,\n",
       "  'sample': 0.0018511172815020495,\n",
       "  'accept': 0.0014544492926087532,\n",
       "  'put': 0.00436334787782626,\n",
       "  'conclude': 0.0030411212481819385,\n",
       "  'cluster': 0.0002644453259288642,\n",
       "  'group': 0.0006611133148221605,\n",
       "  'a': 0.0001322226629644321,\n",
       "  'restrict': 0.0002644453259288642,\n",
       "  'learn': 0.0018511172815020495,\n",
       "  'pre': 0.0001322226629644321,\n",
       "  'request': 0.0001322226629644321,\n",
       "  'need': 0.0009255586407510248,\n",
       "  'exclude': 0.002247785270395346,\n",
       "  'ignore': 0.0015866719555731853,\n",
       "  'visualize': 0.0003966679888932963,\n",
       "  'feed': 0.0001322226629644321,\n",
       "  'expect': 0.005950019833399445,\n",
       "  'leave': 0.0018511172815020495,\n",
       "  ':': 0.001322226629644321,\n",
       "  'review': 0.0001322226629644321,\n",
       "  'trust': 0.004627793203755124,\n",
       "  'collect': 0.001190003966679889,\n",
       "  'instead': 0.002247785270395346,\n",
       "  'tune': 0.0006611133148221605,\n",
       "  'estimate': 0.0017188946185376173,\n",
       "  'characterize': 0.0002644453259288642,\n",
       "  'weight': 0.0006611133148221605,\n",
       "  'guess': 0.0005288906518577285,\n",
       "  'call': 0.002644453259288642,\n",
       "  'write': 0.0033055665741108025,\n",
       "  'get': 0.002380007933359778,\n",
       "  'even': 0.001322226629644321,\n",
       "  'note': 0.0002644453259288642,\n",
       "  'simply': 0.004098902551897396,\n",
       "  'invert': 0.0001322226629644321,\n",
       "  'indicate': 0.0003966679888932963,\n",
       "  'believe': 0.001190003966679889,\n",
       "  'arrange': 0.0005288906518577285,\n",
       "  'simulate': 0.0005288906518577285,\n",
       "  'reduce': 0.0005288906518577285,\n",
       "  'respond': 0.0003966679888932963,\n",
       "  ')': 0.0003966679888932963,\n",
       "  'forecast': 0.0002644453259288642,\n",
       "  'fill': 0.0005288906518577285,\n",
       "  'repeat': 0.0009255586407510248,\n",
       "  'compute': 0.004098902551897396,\n",
       "  'manually': 0.0003966679888932963,\n",
       "  'remove': 0.004892238529683988,\n",
       "  'impute': 0.001057781303715457,\n",
       "  'wait': 0.0002644453259288642,\n",
       "  'open': 0.0003966679888932963,\n",
       "  'specify': 0.0019833399444664813,\n",
       "  'summarize': 0.0003966679888932963,\n",
       "  'avoid': 0.0014544492926087532,\n",
       "  'address': 0.0006611133148221605,\n",
       "  'also': 0.004627793203755124,\n",
       "  'keep': 0.004627793203755124,\n",
       "  'draw': 0.001057781303715457,\n",
       "  'randomly': 0.0007933359777865926,\n",
       "  'prefer': 0.005156683855612852,\n",
       "  'scale': 0.0005288906518577285,\n",
       "  'valuate': 0.0001322226629644321,\n",
       "  'recover': 0.0001322226629644321,\n",
       "  'reinit': 0.0001322226629644321,\n",
       "  'ask': 0.0033055665741108025,\n",
       "  'best': 0.0015866719555731853,\n",
       "  'identify': 0.0003966679888932963,\n",
       "  'simple': 0.0001322226629644321,\n",
       "  'smooth': 0.0003966679888932963,\n",
       "  'hire': 0.0001322226629644321,\n",
       "  'normalize': 0.00251223059632421,\n",
       "  'present': 0.001190003966679889,\n",
       "  'actually': 0.0009255586407510248,\n",
       "  'utilize': 0.0003966679888932963,\n",
       "  'consult': 0.0001322226629644321,\n",
       "  'let': 0.001057781303715457,\n",
       "  'enforce': 0.0001322226629644321,\n",
       "  'classify': 0.0003966679888932963,\n",
       "  'type': 0.0005288906518577285,\n",
       "  ',': 0.001190003966679889,\n",
       "  'purge': 0.0001322226629644321,\n",
       "  'repeatedly': 0.0001322226629644321,\n",
       "  'allow': 0.0007933359777865926,\n",
       "  'divide': 0.002247785270395346,\n",
       "  'therefore': 0.0003966679888932963,\n",
       "  'pay': 0.0009255586407510248,\n",
       "  'organize': 0.0003966679888932963,\n",
       "  'self': 0.0001322226629644321,\n",
       "  'deliver': 0.0001322226629644321,\n",
       "  'replace': 0.001057781303715457,\n",
       "  'discard': 0.001057781303715457,\n",
       "  'work': 0.0005288906518577285,\n",
       "  'label': 0.0001322226629644321,\n",
       "  'move': 0.0005288906518577285,\n",
       "  'prepare': 0.0002644453259288642,\n",
       "  'input': 0.0002644453259288642,\n",
       "  'follow': 0.002644453259288642,\n",
       "  'reflect': 0.0001322226629644321,\n",
       "  '...': 0.0001322226629644321,\n",
       "  'artificially': 0.0001322226629644321,\n",
       "  'market': 0.0001322226629644321,\n",
       "  'refer': 0.0015866719555731853,\n",
       "  'form': 0.0001322226629644321,\n",
       "  'remedy': 0.0001322226629644321,\n",
       "  'sum': 0.0005288906518577285,\n",
       "  'assess': 0.0009255586407510248,\n",
       "  'bid': 0.0002644453259288642,\n",
       "  'retry': 0.0001322226629644321,\n",
       "  'turn': 0.0005288906518577285,\n",
       "  'prove': 0.0002644453259288642,\n",
       "  'structure': 0.0003966679888932963,\n",
       "  'carry': 0.0006611133148221605,\n",
       "  'adhere': 0.0001322226629644321,\n",
       "  'adopt': 0.001057781303715457,\n",
       "  'regard': 0.0001322226629644321,\n",
       "  'exploit': 0.0001322226629644321,\n",
       "  'at': 0.0001322226629644321,\n",
       "  'investigate': 0.0009255586407510248,\n",
       "  'explain': 0.0007933359777865926,\n",
       "  'still': 0.003966679888932963,\n",
       "  'modify': 0.001190003966679889,\n",
       "  'mention': 0.0005288906518577285,\n",
       "  'code': 0.0005288906518577285,\n",
       "  'aggregate': 0.0003966679888932963,\n",
       "  'pool': 0.0003966679888932963,\n",
       "  'assume': 0.002644453259288642,\n",
       "  'somehow': 0.0009255586407510248,\n",
       "  'count': 0.0006611133148221605,\n",
       "  'standardize': 0.001322226629644321,\n",
       "  'infer': 0.0002644453259288642,\n",
       "  'back': 0.0001322226629644321,\n",
       "  'rely': 0.001190003966679889,\n",
       "  'average': 0.0014544492926087532,\n",
       "  'change': 0.004098902551897396,\n",
       "  'pursue': 0.0005288906518577285,\n",
       "  'separate': 0.0006611133148221605,\n",
       "  'reason': 0.0001322226629644321,\n",
       "  '?': 0.0018511172815020495,\n",
       "  'play': 0.0001322226629644321,\n",
       "  'tackle': 0.0001322226629644321,\n",
       "  'bet': 0.0001322226629644321,\n",
       "  'enter': 0.0005288906518577285,\n",
       "  'rate': 0.0001322226629644321,\n",
       "  'bite': 0.0001322226629644321,\n",
       "  'now': 0.0007933359777865926,\n",
       "  'detect': 0.0001322226629644321,\n",
       "  'Correct': 0.0001322226629644321,\n",
       "  'state': 0.0003966679888932963,\n",
       "  'reject': 0.0009255586407510248,\n",
       "  '(': 0.001057781303715457,\n",
       "  'recommend': 0.0002644453259288642,\n",
       "  'sell': 0.0002644453259288642,\n",
       "  'roll': 0.0001322226629644321,\n",
       "  'reverse': 0.0001322226629644321,\n",
       "  'loop': 0.0001322226629644321,\n",
       "  'over': 0.0001322226629644321,\n",
       "  'used': 0.0015866719555731853,\n",
       "  'see': 0.0006611133148221605,\n",
       "  'establish': 0.0003966679888932963,\n",
       "  'insert': 0.0005288906518577285,\n",
       "  'process': 0.0002644453259288642,\n",
       "  'efficiently': 0.0002644453259288642,\n",
       "  'generate': 0.0007933359777865926,\n",
       "  'research': 0.0002644453259288642,\n",
       "  'interpred': 0.0002644453259288642,\n",
       "  'abandon': 0.0003966679888932963,\n",
       "  'bin': 0.0002644453259288642,\n",
       "  'correctly': 0.0003966679888932963,\n",
       "  'syntax': 0.0001322226629644321,\n",
       "  'predict': 0.0005288906518577285,\n",
       "  'completely': 0.0002644453259288642,\n",
       "  'download': 0.0001322226629644321,\n",
       "  'further': 0.0001322226629644321,\n",
       "  'un': 0.0001322226629644321,\n",
       "  'formalize': 0.0002644453259288642,\n",
       "  'stop': 0.0014544492926087532,\n",
       "  'discern': 0.0001322226629644321,\n",
       "  'break': 0.0003966679888932963,\n",
       "  'optimally': 0.0001322226629644321,\n",
       "  'blindly': 0.0001322226629644321,\n",
       "  'know': 0.001322226629644321,\n",
       "  'for': 0.0006611133148221605,\n",
       "  're': 0.001322226629644321,\n",
       "  'increment': 0.0001322226629644321,\n",
       "  'attempt': 0.0001322226629644321,\n",
       "  'refresh': 0.0001322226629644321,\n",
       "  'come': 0.0003966679888932963,\n",
       "  'train': 0.001057781303715457,\n",
       "  'attach': 0.0001322226629644321,\n",
       "  'dive': 0.0002644453259288642,\n",
       "  'explore': 0.0002644453259288642,\n",
       "  'increase': 0.0007933359777865926,\n",
       "  'drop': 0.001190003966679889,\n",
       "  'omit': 0.0002644453259288642,\n",
       "  'really': 0.0007933359777865926,\n",
       "  '\"': 0.001190003966679889,\n",
       "  'subset': 0.0001322226629644321,\n",
       "  'improve': 0.0003966679888932963,\n",
       "  'necessarily': 0.0003966679888932963,\n",
       "  'name': 0.0001322226629644321,\n",
       "  'cache': 0.0001322226629644321,\n",
       "  'built': 0.0002644453259288642,\n",
       "  'phrase': 0.0001322226629644321,\n",
       "  'nail': 0.0001322226629644321,\n",
       "  'deduce': 0.0002644453259288642,\n",
       "  'fix': 0.0007933359777865926,\n",
       "  'upsample': 0.0001322226629644321,\n",
       "  'implement': 0.001190003966679889,\n",
       "  'watch': 0.0001322226629644321,\n",
       "  'split': 0.0009255586407510248,\n",
       "  'quantify': 0.0001322226629644321,\n",
       "  'suppose': 0.0001322226629644321,\n",
       "  'delve': 0.0001322226629644321,\n",
       "  'integrate': 0.0007933359777865926,\n",
       "  'properly': 0.0003966679888932963,\n",
       "  'segregate': 0.0001322226629644321,\n",
       "  'concern': 0.0001322226629644321,\n",
       "  'pass': 0.0006611133148221605,\n",
       "  'filter': 0.0003966679888932963,\n",
       "  'manupliate': 0.0001322226629644321,\n",
       "  'incorporate': 0.0007933359777865926,\n",
       "  'formulate': 0.001057781303715457,\n",
       "  'place': 0.0003966679888932963,\n",
       "  'encode': 0.0003966679888932963,\n",
       "  'resample': 0.0003966679888932963,\n",
       "  'begin': 0.0005288906518577285,\n",
       "  'progress': 0.0002644453259288642,\n",
       "  'interview': 0.0001322226629644321,\n",
       "  'extract': 0.0003966679888932963,\n",
       "  'represent': 0.001057781303715457,\n",
       "  'invest': 0.0003966679888932963,\n",
       "  'aim': 0.0002644453259288642,\n",
       "  'preprocess': 0.0003966679888932963,\n",
       "  'distribute': 0.0002644453259288642,\n",
       "  'stratify': 0.0001322226629644321,\n",
       "  'separately': 0.0001322226629644321,\n",
       "  'dummy': 0.0002644453259288642,\n",
       "  '/': 0.0003966679888932963,\n",
       "  'interpreted': 0.0003966679888932963,\n",
       "  'log': 0.0005288906518577285,\n",
       "  'scal': 0.0001322226629644321,\n",
       "  'differentiate': 0.0002644453259288642,\n",
       "  'reasonably': 0.0001322226629644321,\n",
       "  'resolve': 0.0001322226629644321,\n",
       "  'introduce': 0.0003966679888932963,\n",
       "  'procedd': 0.0001322226629644321,\n",
       "  'dismiss': 0.0001322226629644321,\n",
       "  'configure': 0.0002644453259288642,\n",
       "  'plan': 0.0001322226629644321,\n",
       "  'throw': 0.0002644453259288642,\n",
       "  'show': 0.0002644453259288642,\n",
       "  'minimize': 0.0006611133148221605,\n",
       "  'provide': 0.0015866719555731853,\n",
       "  'strip': 0.0001322226629644321,\n",
       "  'annualize': 0.0001322226629644321,\n",
       "  'bother': 0.0001322226629644321,\n",
       "  'trim': 0.0001322226629644321,\n",
       "  'verify': 0.0001322226629644321,\n",
       "  'tread': 0.0001322226629644321,\n",
       "  'analysis': 0.0001322226629644321,\n",
       "  'obfuscate': 0.0001322226629644321,\n",
       "  'merge': 0.0005288906518577285,\n",
       "  'center': 0.0002644453259288642,\n",
       "  'de': 0.0001322226629644321,\n",
       "  'evaluate': 0.0009255586407510248,\n",
       "  'log2': 0.0001322226629644321,\n",
       "  'categorize': 0.0001322226629644321,\n",
       "  'exactly': 0.0003966679888932963,\n",
       "  'debug': 0.0001322226629644321,\n",
       "  'forget': 0.0002644453259288642,\n",
       "  'live': 0.0001322226629644321,\n",
       "  'update': 0.0005288906518577285,\n",
       "  'resort': 0.0001322226629644321,\n",
       "  'specifiy': 0.0001322226629644321,\n",
       "  'initialize': 0.0005288906518577285,\n",
       "  'convince': 0.0001322226629644321,\n",
       "  'express': 0.0003966679888932963,\n",
       "  'judge': 0.0001322226629644321,\n",
       "  'convert': 0.001322226629644321,\n",
       "  'confirm': 0.0002644453259288642,\n",
       "  'suspect': 0.0001322226629644321,\n",
       "  'one': 0.0001322226629644321,\n",
       "  'reshuffle': 0.0001322226629644321,\n",
       "  'sit': 0.0001322226629644321,\n",
       "  'evalute': 0.0001322226629644321,\n",
       "  'consume': 0.0001322226629644321,\n",
       "  'parallelize': 0.0001322226629644321,\n",
       "  'am': 0.0001322226629644321,\n",
       "  'by': 0.0001322226629644321,\n",
       "  'analytically': 0.0001322226629644321,\n",
       "  'manage': 0.0001322226629644321,\n",
       "  'examine': 0.0003966679888932963,\n",
       "  'generally': 0.0002644453259288642,\n",
       "  'anyway': 0.0001322226629644321,\n",
       "  'sub': 0.0001322226629644321,\n",
       "  'sum(compare.data[,\"predict': 0.0001322226629644321,\n",
       "  'literally': 0.0001322226629644321,\n",
       "  'interprete': 0.0001322226629644321,\n",
       "  'NOT': 0.0001322226629644321,\n",
       "  'memorize': 0.0001322226629644321,\n",
       "  'rule': 0.0001322226629644321,\n",
       "  'in': 0.0001322226629644321,\n",
       "  'recognize': 0.0003966679888932963,\n",
       "  'use-': 0.0001322226629644321,\n",
       "  'oversample': 0.0001322226629644321,\n",
       "  'force': 0.0003966679888932963,\n",
       "  'perhaps': 0.0002644453259288642,\n",
       "  'simplify': 0.0001322226629644321,\n",
       "  'statistically': 0.0001322226629644321,\n",
       "  'post': 0.0018511172815020495,\n",
       "  'seek': 0.0002644453259288642,\n",
       "  'derive': 0.0002644453259288642,\n",
       "  'link': 0.0001322226629644321,\n",
       "  'conceive': 0.0001322226629644321,\n",
       "  'mark': 0.0001322226629644321,\n",
       "  'factor': 0.0002644453259288642,\n",
       "  'notice': 0.0001322226629644321,\n",
       "  'operate': 0.0001322226629644321,\n",
       "  'cater': 0.0001322226629644321,\n",
       "  'procede': 0.0002644453259288642,\n",
       "  'graph': 0.0002644453259288642,\n",
       "  'rank': 0.0001322226629644321,\n",
       "  'appropriately': 0.0001322226629644321,\n",
       "  'bootstrap': 0.0002644453259288642,\n",
       "  'recalculate': 0.0001322226629644321,\n",
       "  'preprocessing': 0.0001322226629644321,\n",
       "  'space': 0.0002644453259288642,\n",
       "  'match': 0.0001322226629644321,\n",
       "  'allocate': 0.0002644453259288642,\n",
       "  'shape': 0.0001322226629644321,\n",
       "  'user': 0.0001322226629644321,\n",
       "  'mean': 0.0001322226629644321,\n",
       "  'join': 0.0001322226629644321,\n",
       "  'ban': 0.0001322226629644321,\n",
       "  'drop?70': 0.0001322226629644321,\n",
       "  'concatenate': 0.0001322226629644321,\n",
       "  'monitor': 0.0001322226629644321,\n",
       "  'quote': 0.0001322226629644321,\n",
       "  'fine': 0.0001322226629644321,\n",
       "  'act': 0.0003966679888932963,\n",
       "  'revise': 0.0002644453259288642,\n",
       "  'training': 0.0001322226629644321,\n",
       "  'plant': 0.0001322226629644321,\n",
       "  'rescale': 0.0002644453259288642,\n",
       "  'disregard': 0.0001322226629644321,\n",
       "  'instrument': 0.0001322226629644321,\n",
       "  'difference': 0.0002644453259288642,\n",
       "  'decrease': 0.0001322226629644321,\n",
       "  'figure': 0.0003966679888932963,\n",
       "  'react': 0.0001322226629644321,\n",
       "  'record': 0.0001322226629644321,\n",
       "  'calcualte': 0.0001322226629644321,\n",
       "  'refit': 0.0001322226629644321,\n",
       "  'prune': 0.0001322226629644321,\n",
       "  'push': 0.0001322226629644321,\n",
       "  'systematically': 0.0001322226629644321,\n",
       "  'translate': 0.0001322226629644321,\n",
       "  'radnomly': 0.0001322226629644321,\n",
       "  'reliably': 0.0001322226629644321,\n",
       "  'format': 0.0002644453259288642,\n",
       "  'optimize': 0.0001322226629644321,\n",
       "  'normalised': 0.0001322226629644321,\n",
       "  'amend': 0.0001322226629644321,\n",
       "  'switch': 0.0002644453259288642,\n",
       "  'minimise': 0.0001322226629644321,\n",
       "  'stipulate': 0.0001322226629644321,\n",
       "  'standardise': 0.0001322226629644321,\n",
       "  'iterate': 0.0001322226629644321,\n",
       "  'standardized': 0.0001322226629644321,\n",
       "  'tell': 0.0002644453259288642,\n",
       "  'balance': 0.0002644453259288642,\n",
       "  'opt': 0.0002644453259288642,\n",
       "  'underfit': 0.0001322226629644321,\n",
       "  'mistrust': 0.0001322226629644321,\n",
       "  'i': 0.0001322226629644321,\n",
       "  'picture': 0.0001322226629644321,\n",
       "  'differenciate': 0.0002644453259288642,\n",
       "  'overcome': 0.0001322226629644321,\n",
       "  'as': 0.0001322226629644321,\n",
       "  'tidy': 0.0001322226629644321,\n",
       "  'done': 0.0001322226629644321,\n",
       "  'connect': 0.0001322226629644321,\n",
       "  'stay': 0.0002644453259288642,\n",
       "  'correlate': 0.0002644453259288642,\n",
       "  'buy': 0.0005288906518577285,\n",
       "  'justify': 0.0002644453259288642,\n",
       "  'number': 0.0001322226629644321,\n",
       "  'weigh': 0.0001322226629644321,\n",
       "  'tab': 0.0002644453259288642,\n",
       "  'end': 0.0001322226629644321,\n",
       "  'return': 0.0003966679888932963,\n",
       "  'reformulate': 0.0001322226629644321,\n",
       "  'analise': 0.0001322226629644321,\n",
       "  'retain': 0.0001322226629644321,\n",
       "  'compile': 0.0001322226629644321,\n",
       "  'shuffle': 0.0001322226629644321,\n",
       "  'better': 0.0002644453259288642,\n",
       "  'release': 0.0001322226629644321,\n",
       "  'roughly': 0.0001322226629644321,\n",
       "  'recode': 0.0002644453259288642,\n",
       "  'concentrate': 0.0001322226629644321,\n",
       "  'extend': 0.0001322226629644321,\n",
       "  'cover': 0.0001322226629644321,\n",
       "  'map': 0.0001322226629644321,\n",
       "  'rephrase': 0.0003966679888932963,\n",
       "  'from': 0.0001322226629644321,\n",
       "  'or': 0.0002644453259288642,\n",
       "  'edit': 0.001322226629644321,\n",
       "  'discuss': 0.0001322226629644321,\n",
       "  'condition': 0.0001322226629644321,\n",
       "  'target': 0.0001322226629644321,\n",
       "  'relax': 0.0001322226629644321,\n",
       "  'setup': 0.0001322226629644321,\n",
       "  'was': 0.0001322226629644321,\n",
       "  'limit': 0.0001322226629644321,\n",
       "  'want': 0.0001322226629644321,\n",
       "  'validate': 0.0001322226629644321,\n",
       "  'produce': 0.0001322226629644321,\n",
       "  'report?': 0.0001322226629644321,\n",
       "  'penalize': 0.0001322226629644321,\n",
       "  'fitt': 0.0001322226629644321,\n",
       "  'rewrite': 0.0001322226629644321,\n",
       "  'violate': 0.0001322226629644321,\n",
       "  'bucket': 0.0001322226629644321,\n",
       "  'acquire': 0.0002644453259288642,\n",
       "  'proceed?Can': 0.0001322226629644321,\n",
       "  'suggest': 0.0002644453259288642,\n",
       "  'redo': 0.0001322226629644321,\n",
       "  'thus': 0.0001322226629644321,\n",
       "  'skip': 0.0002644453259288642,\n",
       "  'strive': 0.0002644453259288642,\n",
       "  'clarify': 0.0003966679888932963,\n",
       "  'designate': 0.0001322226629644321,\n",
       "  'nest': 0.0001322226629644321,\n",
       "  'cross': 0.0001322226629644321,\n",
       "  'feel': 0.0001322226629644321,\n",
       "  'interact': 0.0001322226629644321,\n",
       "  'plug': 0.0001322226629644321,\n",
       "  'choose??.Either': 0.0001322226629644321,\n",
       "  'google': 0.0001322226629644321,\n",
       "  'impose': 0.0001322226629644321,\n",
       "  'marginalize': 0.0001322226629644321,\n",
       "  'wear': 0.0001322226629644321,\n",
       "  'share': 0.0001322226629644321,\n",
       "  'undertake': 0.0001322226629644321,\n",
       "  'visualise': 0.0001322226629644321,\n",
       "  'send': 0.0001322226629644321,\n",
       "  'denormolize': 0.0001322226629644321,\n",
       "  'base': 0.0001322226629644321,\n",
       "  'close': 0.0002644453259288642,\n",
       "  'bootrstap': 0.0001322226629644321,\n",
       "  'collate': 0.0001322226629644321,\n",
       "  'offer': 0.0001322226629644321,\n",
       "  'den': 0.0001322226629644321,\n",
       "  'arbitrarily': 0.0001322226629644321,\n",
       "  'arbitrate': 0.0001322226629644321,\n",
       "  'elaborate': 0.0002644453259288642,\n",
       "  'program': 0.0001322226629644321,\n",
       "  'loose': 0.0001322226629644321,\n",
       "  'expand': 0.0002644453259288642,\n",
       "  'communicate': 0.0001322226629644321,\n",
       "  'undo': 0.0001322226629644321,\n",
       "  'vary': 0.0001322226629644321,\n",
       "  'ping': 0.0001322226629644321,\n",
       "  'directly': 0.0002644453259288642,\n",
       "  'presume': 0.0001322226629644321,\n",
       "  'inspect': 0.0001322226629644321,\n",
       "  'ideally': 0.0001322226629644321,\n",
       "  'partition': 0.0001322226629644321,\n",
       "  'imagine': 0.0001322226629644321,\n",
       "  'atleast': 0.0001322226629644321,\n",
       "  'chalk': 0.0001322226629644321,\n",
       "  'specifically': 0.0001322226629644321,\n",
       "  'do?This': 0.0001322226629644321,\n",
       "  'covert': 0.0001322226629644321,\n",
       "  'import': 0.0001322226629644321,\n",
       "  'relate': 0.0002644453259288642,\n",
       "  'deadl': 0.0001322226629644321,\n",
       "  'discount': 0.0001322226629644321,\n",
       "  'cite': 0.0002644453259288642,\n",
       "  'erase': 0.0001322226629644321,\n",
       "  'slice': 0.0001322226629644321,\n",
       "  'lose': 0.0001322226629644321,\n",
       "  'fist': 0.0001322226629644321,\n",
       "  'tweak': 0.0001322226629644321,\n",
       "  'rethink': 0.0001322226629644321,\n",
       "  'resume': 0.0001322226629644321,\n",
       "  'batch': 0.0002644453259288642,\n",
       "  'never': 0.0001322226629644321,\n",
       "  'talk': 0.0001322226629644321,\n",
       "  'realise': 0.0001322226629644321,\n",
       "  'reengage': 0.0001322226629644321,\n",
       "  'supply': 0.0001322226629644321,\n",
       "  '[': 0.0001322226629644321,\n",
       "  'reshape': 0.0001322226629644321,\n",
       "  'heed': 0.0001322226629644321},\n",
       " {'prior': 1.0},\n",
       " {'distributions': 1.0},\n",
       " {'from': 0.00980392156862745,\n",
       "  'can': 0.0196078431372549,\n",
       "  'for': 0.11764705882352941,\n",
       "  'in': 0.03431372549019608,\n",
       "  'that': 0.03431372549019608,\n",
       "  \"'\": 0.004901960784313725,\n",
       "  'together': 0.004901960784313725,\n",
       "  'on': 0.03431372549019608,\n",
       "  'look': 0.004901960784313725,\n",
       "  ')': 0.0196078431372549,\n",
       "  '?': 0.03431372549019608,\n",
       "  ',': 0.1323529411764706,\n",
       "  ':': 0.00980392156862745,\n",
       "  '.': 0.1715686274509804,\n",
       "  'be': 0.004901960784313725,\n",
       "  'there': 0.004901960784313725,\n",
       "  'to': 0.029411764705882353,\n",
       "  'are': 0.058823529411764705,\n",
       "  'would': 0.004901960784313725,\n",
       "  'whereas': 0.004901960784313725,\n",
       "  'should': 0.004901960784313725,\n",
       "  'do': 0.004901960784313725,\n",
       "  '(': 0.0196078431372549,\n",
       "  'is': 0.004901960784313725,\n",
       "  'then': 0.004901960784313725,\n",
       "  'but': 0.004901960784313725,\n",
       "  'of': 0.014705882352941176,\n",
       "  'just': 0.00980392156862745,\n",
       "  'were': 0.014705882352941176,\n",
       "  'with': 0.014705882352941176,\n",
       "  'and': 0.0196078431372549,\n",
       "  'over': 0.00980392156862745,\n",
       "  'by': 0.014705882352941176,\n",
       "  'associated': 0.004901960784313725,\n",
       "  'have': 0.00980392156862745,\n",
       "  'I': 0.00980392156862745,\n",
       "  'used': 0.004901960784313725,\n",
       "  'according': 0.004901960784313725,\n",
       "  '\"': 0.00980392156862745,\n",
       "  'themselves': 0.004901960784313725,\n",
       "  'at': 0.00980392156862745,\n",
       "  'given': 0.004901960784313725,\n",
       "  'chosen': 0.004901960784313725,\n",
       "  'called': 0.004901960784313725,\n",
       "  'which': 0.00980392156862745,\n",
       "  'you': 0.00980392156862745,\n",
       "  'onto': 0.004901960784313725,\n",
       "  'defined': 0.004901960784313725,\n",
       "  'and/or': 0.004901960784313725,\n",
       "  'we': 0.004901960784313725,\n",
       "  'when': 0.004901960784313725},\n",
       " {'experts': 0.009009009009009009,\n",
       "  'D': 0.009009009009009009,\n",
       "  'two': 0.018018018018018018,\n",
       "  'scratch': 0.009009009009009009,\n",
       "  'which': 0.16216216216216217,\n",
       "  'a': 0.0990990990990991,\n",
       "  'the': 0.2072072072072072,\n",
       "  'normal': 0.009009009009009009,\n",
       "  'what': 0.009009009009009009,\n",
       "  'histograms': 0.018018018018018018,\n",
       "  'one': 0.018018018018018018,\n",
       "  'some': 0.02702702702702703,\n",
       "  'both': 0.009009009009009009,\n",
       "  'data': 0.018018018018018018,\n",
       "  'separate': 0.009009009009009009,\n",
       "  'first': 0.009009009009009009,\n",
       "  'scattered': 0.009009009009009009,\n",
       "  'to': 0.009009009009009009,\n",
       "  'each': 0.009009009009009009,\n",
       "  'statistical': 0.009009009009009009,\n",
       "  'this': 0.036036036036036036,\n",
       "  'other': 0.009009009009009009,\n",
       "  'them': 0.018018018018018018,\n",
       "  'A': 0.009009009009009009,\n",
       "  'copulas': 0.018018018018018018,\n",
       "  'left': 0.009009009009009009,\n",
       "  'bootstrapping': 0.009009009009009009,\n",
       "  'large': 0.009009009009009009,\n",
       "  'your': 0.018018018018018018,\n",
       "  'Haynes': 0.009009009009009009,\n",
       "  'knowledge': 0.009009009009009009,\n",
       "  'that': 0.009009009009009009,\n",
       "  'ordinal': 0.009009009009009009,\n",
       "  'resulting': 0.009009009009009009,\n",
       "  'their': 0.018018018018018018,\n",
       "  'random': 0.009009009009009009,\n",
       "  'uniform': 0.009009009009009009,\n",
       "  'simulation': 0.009009009009009009,\n",
       "  'these': 0.009009009009009009,\n",
       "  'sampled': 0.009009009009009009,\n",
       "  'clients': 0.009009009009009009,\n",
       "  'store': 0.009009009009009009,\n",
       "  'linear': 0.009009009009009009,\n",
       "  'my': 0.009009009009009009,\n",
       "  'marginals': 0.009009009009009009,\n",
       "  'scipy': 0.009009009009009009,\n",
       "  'joint': 0.009009009009009009,\n",
       "  'different': 0.009009009009009009,\n",
       "  'independent': 0.009009009009009009},\n",
       " {'when': 0.029411764705882353,\n",
       "  'What': 0.029411764705882353,\n",
       "  'whereas': 0.029411764705882353,\n",
       "  'in': 0.14705882352941177,\n",
       "  'if': 0.029411764705882353,\n",
       "  '(': 0.058823529411764705,\n",
       "  ',': 0.08823529411764706,\n",
       "  'here': 0.029411764705882353,\n",
       "  '.': 0.23529411764705882,\n",
       "  'on': 0.029411764705882353,\n",
       "  ':)': 0.029411764705882353,\n",
       "  'using': 0.029411764705882353,\n",
       "  'calibrated': 0.029411764705882353,\n",
       "  'as': 0.029411764705882353,\n",
       "  'like': 0.08823529411764706,\n",
       "  'to': 0.029411764705882353,\n",
       "  'and': 0.058823529411764705},\n",
       " {'fitting': 1.0},\n",
       " {'a': 0.2845849802371542,\n",
       "  '(': 0.003952569169960474,\n",
       "  'other': 0.003952569169960474,\n",
       "  'mixed': 0.007905138339920948,\n",
       "  'GAM': 0.003952569169960474,\n",
       "  'with': 0.011857707509881422,\n",
       "  'the': 0.3201581027667984,\n",
       "  'higher': 0.003952569169960474,\n",
       "  '.': 0.023715415019762844,\n",
       "  'an': 0.03162055335968379,\n",
       "  'that': 0.007905138339920948,\n",
       "  'vectors': 0.003952569169960474,\n",
       "  'ARIMA': 0.003952569169960474,\n",
       "  'data': 0.011857707509881422,\n",
       "  'SEM': 0.003952569169960474,\n",
       "  'model': 0.015810276679841896,\n",
       "  'linear': 0.007905138339920948,\n",
       "  'interactions': 0.011857707509881422,\n",
       "  'to': 0.003952569169960474,\n",
       "  ',': 0.003952569169960474,\n",
       "  'GLMMs': 0.003952569169960474,\n",
       "  'and': 0.007905138339920948,\n",
       "  'your': 0.019762845849802372,\n",
       "  'growth': 0.003952569169960474,\n",
       "  'natural': 0.003952569169960474,\n",
       "  'regressions': 0.003952569169960474,\n",
       "  'ML': 0.003952569169960474,\n",
       "  'GAMs': 0.007905138339920948,\n",
       "  'polynomials': 0.003952569169960474,\n",
       "  'coxph': 0.003952569169960474,\n",
       "  'LMMs': 0.003952569169960474,\n",
       "  'lasso': 0.003952569169960474,\n",
       "  'on': 0.007905138339920948,\n",
       "  'multivariate': 0.003952569169960474,\n",
       "  'models': 0.015810276679841896,\n",
       "  'no': 0.003952569169960474,\n",
       "  'machine': 0.003952569169960474,\n",
       "  'multilevel': 0.003952569169960474,\n",
       "  'this': 0.003952569169960474,\n",
       "  'regularized': 0.003952569169960474,\n",
       "  'lighter': 0.003952569169960474,\n",
       "  'for': 0.003952569169960474,\n",
       "  'truncated': 0.003952569169960474,\n",
       "  'without': 0.003952569169960474,\n",
       "  'polynomial': 0.003952569169960474,\n",
       "  'their': 0.003952569169960474,\n",
       "  'these': 0.007905138339920948,\n",
       "  ':': 0.003952569169960474,\n",
       "  'Poisson': 0.003952569169960474,\n",
       "  \"'\": 0.003952569169960474,\n",
       "  'even': 0.003952569169960474,\n",
       "  'GLMs': 0.003952569169960474,\n",
       "  'AR(1': 0.003952569169960474,\n",
       "  'at': 0.003952569169960474,\n",
       "  'RF': 0.003952569169960474,\n",
       "  'several': 0.003952569169960474,\n",
       "  'smooth': 0.003952569169960474,\n",
       "  '?': 0.003952569169960474,\n",
       "  'two': 0.003952569169960474,\n",
       "  'neural': 0.003952569169960474,\n",
       "  '...': 0.003952569169960474,\n",
       "  'any': 0.003952569169960474,\n",
       "  'power': 0.003952569169960474,\n",
       "  'Gaussian': 0.003952569169960474,\n",
       "  'autoregressive': 0.003952569169960474,\n",
       "  'lognormal': 0.003952569169960474,\n",
       "  'may': 0.003952569169960474},\n",
       " {'Bayesian': 0.00404040404040404,\n",
       "  'Pareto': 0.00101010101010101,\n",
       "  'large': 0.00202020202020202,\n",
       "  'GLM': 0.014141414141414142,\n",
       "  'model': 0.15404040404040403,\n",
       "  'distribution': 0.024747474747474747,\n",
       "  'logistic': 0.03585858585858586,\n",
       "  'small': 0.000505050505050505,\n",
       "  'seasonal': 0.00101010101010101,\n",
       "  'straight': 0.014141414141414142,\n",
       "  '4': 0.00101010101010101,\n",
       "  'simple': 0.012626262626262626,\n",
       "  'binomial': 0.005555555555555556,\n",
       "  'Tweedie': 0.000505050505050505,\n",
       "  'glm': 0.00404040404040404,\n",
       "  'new': 0.005555555555555556,\n",
       "  'single': 0.011111111111111112,\n",
       "  'specific': 0.00101010101010101,\n",
       "  'reduced': 0.000505050505050505,\n",
       "  'conditional': 0.0015151515151515152,\n",
       "  'nested': 0.0025252525252525255,\n",
       "  'multilevel': 0.0025252525252525255,\n",
       "  'priori': 0.000505050505050505,\n",
       "  'Binomial': 0.000505050505050505,\n",
       "  'GLMM': 0.0025252525252525255,\n",
       "  'more': 0.00202020202020202,\n",
       "  'regression': 0.04191919191919192,\n",
       "  'spline': 0.0035353535353535356,\n",
       "  'compound': 0.00101010101010101,\n",
       "  'curve': 0.015656565656565657,\n",
       "  'flexible': 0.00101010101010101,\n",
       "  'mixture': 0.010101010101010102,\n",
       "  'univariate': 0.00202020202020202,\n",
       "  'cubic': 0.004545454545454545,\n",
       "  'response': 0.00101010101010101,\n",
       "  'hyperprior': 0.000505050505050505,\n",
       "  'generalized': 0.009595959595959595,\n",
       "  'mixed': 0.018686868686868686,\n",
       "  'marginal': 0.000505050505050505,\n",
       "  '3-level': 0.000505050505050505,\n",
       "  'natural': 0.00101010101010101,\n",
       "  'multiple': 0.005555555555555556,\n",
       "  'neural': 0.006060606060606061,\n",
       "  'range': 0.00101010101010101,\n",
       "  'variogram': 0.00101010101010101,\n",
       "  'copula': 0.0015151515151515152,\n",
       "  'particular': 0.005050505050505051,\n",
       "  'loess': 0.00202020202020202,\n",
       "  'line': 0.025757575757575757,\n",
       "  'polynomial': 0.011616161616161616,\n",
       "  'linear': 0.07575757575757576,\n",
       "  'normal': 0.014646464646464647,\n",
       "  'statistical': 0.000505050505050505,\n",
       "  'quasi': 0.000505050505050505,\n",
       "  'second': 0.00202020202020202,\n",
       "  'hierarchical': 0.00202020202020202,\n",
       "  'non': 0.00909090909090909,\n",
       "  'CFA': 0.000505050505050505,\n",
       "  '2d': 0.00101010101010101,\n",
       "  'weighted': 0.0015151515151515152,\n",
       "  'negative': 0.005555555555555556,\n",
       "  'least': 0.000505050505050505,\n",
       "  'sigmoid': 0.00202020202020202,\n",
       "  '3-gaussian': 0.000505050505050505,\n",
       "  'random': 0.00909090909090909,\n",
       "  'general': 0.0025252525252525255,\n",
       "  'regular': 0.000505050505050505,\n",
       "  'staionary': 0.000505050505050505,\n",
       "  'gam': 0.0025252525252525255,\n",
       "  'smoother': 0.00101010101010101,\n",
       "  'type': 0.000505050505050505,\n",
       "  'Cox': 0.005555555555555556,\n",
       "  'separate': 0.0035353535353535356,\n",
       "  'tree': 0.0030303030303030303,\n",
       "  'panel': 0.00101010101010101,\n",
       "  'harmonic': 0.00101010101010101,\n",
       "  'quadratic': 0.00808080808080808,\n",
       "  'mean': 0.0015151515151515152,\n",
       "  'horizontal': 0.00101010101010101,\n",
       "  'Polynomial': 0.000505050505050505,\n",
       "  'svm': 0.000505050505050505,\n",
       "  'GEE': 0.0025252525252525255,\n",
       "  'Gaussian': 0.010101010101010102,\n",
       "  'proportional': 0.00101010101010101,\n",
       "  'decision': 0.0025252525252525255,\n",
       "  'GMM': 0.0035353535353535356,\n",
       "  'Poisson': 0.013131313131313131,\n",
       "  'fourier': 0.000505050505050505,\n",
       "  'constant': 0.00202020202020202,\n",
       "  'continuous': 0.0025252525252525255,\n",
       "  'simpler': 0.00101010101010101,\n",
       "  'power': 0.008585858585858586,\n",
       "  'time': 0.005050505050505051,\n",
       "  'function': 0.008585858585858586,\n",
       "  'nonparametric': 0.000505050505050505,\n",
       "  'higher': 0.00101010101010101,\n",
       "  'Random': 0.00202020202020202,\n",
       "  'spatial': 0.000505050505050505,\n",
       "  'gaussian': 0.004545454545454545,\n",
       "  'good': 0.000505050505050505,\n",
       "  'probability': 0.0025252525252525255,\n",
       "  'GAM': 0.006060606060606061,\n",
       "  'Normal': 0.00101010101010101,\n",
       "  'posterior': 0.000505050505050505,\n",
       "  'GARCH': 0.0065656565656565654,\n",
       "  'parameter': 0.00101010101010101,\n",
       "  'MA': 0.00101010101010101,\n",
       "  'meta': 0.000505050505050505,\n",
       "  'kind': 0.00101010101010101,\n",
       "  'heteroschedastic': 0.000505050505050505,\n",
       "  'parametric': 0.00404040404040404,\n",
       "  'dummy': 0.0015151515151515152,\n",
       "  'Weibull': 0.004545454545454545,\n",
       "  'smooth': 0.00202020202020202,\n",
       "  'count': 0.0015151515151515152,\n",
       "  'maximum': 0.000505050505050505,\n",
       "  'completely': 0.0015151515151515152,\n",
       "  'value': 0.000505050505050505,\n",
       "  'PCA': 0.00101010101010101,\n",
       "  'full': 0.0035353535353535356,\n",
       "  'lm': 0.000505050505050505,\n",
       "  'hyperplane': 0.00101010101010101,\n",
       "  'multinomial': 0.0035353535353535356,\n",
       "  'multivariate': 0.00404040404040404,\n",
       "  '1-dimensional': 0.000505050505050505,\n",
       "  '100-order': 0.000505050505050505,\n",
       "  'stationary': 0.00101010101010101,\n",
       "  'classification': 0.000505050505050505,\n",
       "  'set': 0.0030303030303030303,\n",
       "  'lasso': 0.00101010101010101,\n",
       "  'Student': 0.000505050505050505,\n",
       "  'Quadric': 0.000505050505050505,\n",
       "  'penalized': 0.000505050505050505,\n",
       "  'relationship': 0.00202020202020202,\n",
       "  'basic': 0.0015151515151515152,\n",
       "  'quantile': 0.00101010101010101,\n",
       "  'ridge': 0.000505050505050505,\n",
       "  '\"': 0.004545454545454545,\n",
       "  'Low': 0.000505050505050505,\n",
       "  '(': 0.0030303030303030303,\n",
       "  'well': 0.00101010101010101,\n",
       "  '200': 0.000505050505050505,\n",
       "  'super': 0.000505050505050505,\n",
       "  'plane': 0.00404040404040404,\n",
       "  'glm.nb': 0.000505050505050505,\n",
       "  'KDE': 0.0015151515151515152,\n",
       "  'piecewise': 0.0030303030303030303,\n",
       "  'global': 0.000505050505050505,\n",
       "  'truncated': 0.0015151515151515152,\n",
       "  'cox': 0.000505050505050505,\n",
       "  'nonlinear': 0.0035353535353535356,\n",
       "  'lognormal': 0.000505050505050505,\n",
       "  'pre': 0.00101010101010101,\n",
       "  'memory': 0.000505050505050505,\n",
       "  'gamma': 0.0065656565656565654,\n",
       "  'suitable': 0.0015151515151515152,\n",
       "  'trial': 0.000505050505050505,\n",
       "  'signal': 0.000505050505050505,\n",
       "  'survival': 0.0015151515151515152,\n",
       "  'common': 0.000505050505050505,\n",
       "  'uni': 0.000505050505050505,\n",
       "  'multi': 0.0015151515151515152,\n",
       "  'categorical': 0.000505050505050505,\n",
       "  'Gumbel': 0.000505050505050505,\n",
       "  'Lasso': 0.0015151515151515152,\n",
       "  'triangle': 0.000505050505050505,\n",
       "  'timeseries': 0.000505050505050505,\n",
       "  '1st': 0.000505050505050505,\n",
       "  'KNN': 0.000505050505050505,\n",
       "  'parabola': 0.0015151515151515152,\n",
       "  'different': 0.005050505050505051,\n",
       "  'collection': 0.000505050505050505,\n",
       "  'bell': 0.00101010101010101,\n",
       "  'series': 0.0015151515151515152,\n",
       "  'standard': 0.0035353535353535356,\n",
       "  'B': 0.000505050505050505,\n",
       "  'cumulative': 0.000505050505050505,\n",
       "  'binary': 0.00202020202020202,\n",
       "  'ARCH': 0.000505050505050505,\n",
       "  'VAR': 0.00404040404040404,\n",
       "  'regularised': 0.000505050505050505,\n",
       "  'quite': 0.000505050505050505,\n",
       "  'latent': 0.000505050505050505,\n",
       "  'number': 0.00202020202020202,\n",
       "  'rather': 0.000505050505050505,\n",
       "  'VECM': 0.0015151515151515152,\n",
       "  'GEV': 0.0015151515151515152,\n",
       "  'logarithmic': 0.00101010101010101,\n",
       "  'log': 0.00202020202020202,\n",
       "  'family': 0.000505050505050505,\n",
       "  'fixed': 0.0035353535353535356,\n",
       "  'Taylor': 0.000505050505050505,\n",
       "  'structural': 0.0025252525252525255,\n",
       "  'GAMM': 0.000505050505050505,\n",
       "  'trendline': 0.000505050505050505,\n",
       "  'functional': 0.00202020202020202,\n",
       "  'trend': 0.004545454545454545,\n",
       "  'topic': 0.000505050505050505,\n",
       "  'two': 0.00101010101010101,\n",
       "  'one': 0.000505050505050505,\n",
       "  'term': 0.00101010101010101,\n",
       "  'periodic': 0.00101010101010101,\n",
       "  'GP': 0.0025252525252525255,\n",
       "  'fully': 0.000505050505050505,\n",
       "  'bounding': 0.000505050505050505,\n",
       "  'SEM': 0.000505050505050505,\n",
       "  'LLS': 0.000505050505050505,\n",
       "  'ARIMA': 0.00101010101010101,\n",
       "  'glmer.nb': 0.000505050505050505,\n",
       "  '4-deg': 0.000505050505050505,\n",
       "  'population': 0.00101010101010101,\n",
       "  'long': 0.000505050505050505,\n",
       "  'poisson': 0.0025252525252525255,\n",
       "  'Logistic': 0.000505050505050505,\n",
       "  'the': 0.000505050505050505,\n",
       "  'third': 0.000505050505050505,\n",
       "  'tenth': 0.000505050505050505,\n",
       "  'smoothing': 0.00101010101010101,\n",
       "  'LOESS': 0.00101010101010101,\n",
       "  'Ridge': 0.000505050505050505,\n",
       "  'Neural': 0.000505050505050505,\n",
       "  'bunch': 0.0025252525252525255,\n",
       "  'sinusoidal': 0.000505050505050505,\n",
       "  't': 0.0025252525252525255,\n",
       "  '1-component': 0.000505050505050505,\n",
       "  'bivariate': 0.0025252525252525255,\n",
       "  'bulge': 0.000505050505050505,\n",
       "  'SVM': 0.000505050505050505,\n",
       "  'given': 0.00101010101010101,\n",
       "  'confidence': 0.000505050505050505,\n",
       "  'few': 0.00202020202020202,\n",
       "  'Case': 0.000505050505050505,\n",
       "  'weibull': 0.00101010101010101,\n",
       "  'dynamic': 0.000505050505050505,\n",
       "  'rating': 0.000505050505050505,\n",
       "  'surface': 0.000505050505050505,\n",
       "  'AR(p': 0.000505050505050505,\n",
       "  'three': 0.0035353535353535356,\n",
       "  'Weinbull': 0.000505050505050505,\n",
       "  'lot': 0.00101010101010101,\n",
       "  'glmer': 0.000505050505050505,\n",
       "  'regularized': 0.0015151515151515152,\n",
       "  'Copula': 0.000505050505050505,\n",
       "  'geeglm': 0.000505050505050505,\n",
       "  '9th': 0.000505050505050505,\n",
       "  'sliding': 0.000505050505050505,\n",
       "  'beta': 0.005050505050505051,\n",
       "  \"'\": 0.000505050505050505,\n",
       "  'Beta': 0.000505050505050505,\n",
       "  '@#$^': 0.000505050505050505,\n",
       "  'complex': 0.000505050505050505,\n",
       "  '2-D': 0.000505050505050505,\n",
       "  '2nd': 0.000505050505050505,\n",
       "  'gompertz': 0.000505050505050505,\n",
       "  'classical': 0.00101010101010101,\n",
       "  'Cauchy': 0.000505050505050505,\n",
       "  'Lorentzian': 0.000505050505050505,\n",
       "  'Bernoulli': 0.000505050505050505,\n",
       "  'spike': 0.000505050505050505,\n",
       "  'Tobit': 0.000505050505050505,\n",
       "  'boosted': 0.000505050505050505,\n",
       "  'Generalized': 0.000505050505050505,\n",
       "  'degree': 0.00101010101010101,\n",
       "  'many': 0.000505050505050505,\n",
       "  'bayesian': 0.000505050505050505,\n",
       "  'Propensity': 0.000505050505050505,\n",
       "  '`': 0.0015151515151515152,\n",
       "  'logit': 0.00202020202020202,\n",
       "  'classifier': 0.00101010101010101,\n",
       "  'preidctive': 0.000505050505050505,\n",
       "  'predictive': 0.00101010101010101,\n",
       "  'ZIP': 0.000505050505050505,\n",
       "  '20': 0.000505050505050505,\n",
       "  '$': 0.00101010101010101,\n",
       "  'whole': 0.000505050505050505,\n",
       "  'pair': 0.000505050505050505,\n",
       "  'GARCH(1,1': 0.00101010101010101,\n",
       "  'generalised': 0.000505050505050505,\n",
       "  'prior': 0.000505050505050505,\n",
       "  'misspecified': 0.00101010101010101,\n",
       "  'multivariable': 0.000505050505050505,\n",
       "  'sinusoid': 0.000505050505050505,\n",
       "  'Linear': 0.00101010101010101,\n",
       "  'hierarchal': 0.000505050505050505,\n",
       "  'GBM': 0.000505050505050505,\n",
       "  'physical': 0.000505050505050505,\n",
       "  'covariance': 0.0015151515151515152,\n",
       "  'very': 0.0015151515151515152,\n",
       "  '6-parameter': 0.000505050505050505,\n",
       "  'sine': 0.00202020202020202,\n",
       "  'joint': 0.0015151515151515152,\n",
       "  'prediction': 0.000505050505050505,\n",
       "  'to': 0.000505050505050505,\n",
       "  'a': 0.000505050505050505,\n",
       "  'zeta': 0.000505050505050505,\n",
       "  'high': 0.0015151515151515152,\n",
       "  'correlated': 0.000505050505050505,\n",
       "  'probabilistic': 0.00101010101010101,\n",
       "  'HMM': 0.000505050505050505,\n",
       "  'Gamma': 0.00202020202020202,\n",
       "  'sklearn.ensemble.RandomForestRegressor': 0.000505050505050505,\n",
       "  'standardized': 0.000505050505050505,\n",
       "  'couple': 0.0015151515151515152,\n",
       "  'final': 0.000505050505050505,\n",
       "  '2-factor': 0.000505050505050505,\n",
       "  'restricted': 0.00101010101010101,\n",
       "  'known': 0.000505050505050505,\n",
       "  'lowess': 0.00101010101010101,\n",
       "  'RF': 0.00101010101010101,\n",
       "  '4-level': 0.000505050505050505,\n",
       "  'continuously': 0.000505050505050505,\n",
       "  'stright': 0.000505050505050505,\n",
       "  'Guassian': 0.000505050505050505,\n",
       "  'causal': 0.000505050505050505,\n",
       "  'quarterly': 0.000505050505050505,\n",
       "  'Pearson': 0.000505050505050505,\n",
       "  'fifth': 0.000505050505050505,\n",
       "  'monotone': 0.000505050505050505,\n",
       "  '_': 0.000505050505050505,\n",
       "  'Fine': 0.000505050505050505,\n",
       "  'garch': 0.000505050505050505,\n",
       "  'frailty': 0.000505050505050505,\n",
       "  '[': 0.000505050505050505,\n",
       "  'piece': 0.00101010101010101,\n",
       "  'Burr': 0.000505050505050505,\n",
       "  'location': 0.000505050505050505,\n",
       "  'density': 0.00101010101010101,\n",
       "  'candidate': 0.000505050505050505,\n",
       "  'subset': 0.000505050505050505,\n",
       "  'noncollapsible': 0.000505050505050505,\n",
       "  'sigmoidal': 0.0015151515151515152,\n",
       "  'lme': 0.000505050505050505,\n",
       "  'CDF': 0.000505050505050505,\n",
       "  'deterministic': 0.00101010101010101,\n",
       "  'deep': 0.000505050505050505,\n",
       "  '*': 0.00202020202020202,\n",
       "  'fairly': 0.000505050505050505,\n",
       "  'dataset': 0.00101010101010101,\n",
       "  'TS': 0.000505050505050505,\n",
       "  'data': 0.0015151515151515152,\n",
       "  'double': 0.0015151515151515152,\n",
       "  'best': 0.000505050505050505,\n",
       "  'bernoulli': 0.000505050505050505,\n",
       "  'Binomoal': 0.000505050505050505,\n",
       "  'zero': 0.00101010101010101,\n",
       "  'negbinom': 0.000505050505050505,\n",
       "  'plain': 0.000505050505050505,\n",
       "  'repeated': 0.000505050505050505,\n",
       "  'arma': 0.000505050505050505,\n",
       "  'circle': 0.000505050505050505,\n",
       "  '10th': 0.000505050505050505,\n",
       "  'LASSO': 0.000505050505050505,\n",
       "  'similar': 0.000505050505050505,\n",
       "  'zinb': 0.000505050505050505,\n",
       "  'robust': 0.000505050505050505,\n",
       "  'maximal': 0.000505050505050505,\n",
       "  'substantively': 0.000505050505050505,\n",
       "  'bogus': 0.000505050505050505,\n",
       "  'mechanistic': 0.000505050505050505,\n",
       "  'stable': 0.000505050505050505,\n",
       "  'skewed': 0.000505050505050505,\n",
       "  'unified': 0.000505050505050505,\n",
       "  'low': 0.000505050505050505,\n",
       "  'Markov': 0.000505050505050505,\n",
       "  'Cochran': 0.000505050505050505,\n",
       "  'residual': 0.000505050505050505,\n",
       "  'logical': 0.000505050505050505,\n",
       "  'bad': 0.000505050505050505,\n",
       "  'quasipoisson': 0.000505050505050505,\n",
       "  'Kaplan': 0.000505050505050505,\n",
       "  '“': 0.000505050505050505,\n",
       "  'much': 0.000505050505050505,\n",
       "  'SDE': 0.000505050505050505,\n",
       "  'OLS': 0.000505050505050505,\n",
       "  'true': 0.000505050505050505,\n",
       "  'typical': 0.000505050505050505,\n",
       "  'combination': 0.000505050505050505,\n",
       "  'LS': 0.000505050505050505}]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(freq.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91648</th>\n",
       "      <td>91752</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "      <td>post</td>\n",
       "      <td>84</td>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91649</th>\n",
       "      <td>91753</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "      <td>post</td>\n",
       "      <td>138</td>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91650</th>\n",
       "      <td>91754</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>post</td>\n",
       "      <td>191</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91651</th>\n",
       "      <td>91755</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have two groups of data. Each with a differe...</td>\n",
       "      <td>post</td>\n",
       "      <td>477</td>\n",
       "      <td>I have two groups of data . Each with a differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91652</th>\n",
       "      <td>91756</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The R-project R is valuable and significant be...</td>\n",
       "      <td>post</td>\n",
       "      <td>289</td>\n",
       "      <td>The R - project R is valuable and significant ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  post_id  parent_id  comment_id  \\\n",
       "91648       91752        1        NaN         NaN   \n",
       "91649       91753        2        NaN         NaN   \n",
       "91650       91754        3        NaN         NaN   \n",
       "91651       91755        4        NaN         NaN   \n",
       "91652       91756        5        3.0         NaN   \n",
       "\n",
       "                                                    text category  length  \\\n",
       "91648  How should I elicit prior distributions from e...     post      84   \n",
       "91649  In many different statistical methods there is...     post     138   \n",
       "91650  What are some valuable Statistical Analysis op...     post     191   \n",
       "91651  I have two groups of data. Each with a differe...     post     477   \n",
       "91652  The R-project R is valuable and significant be...     post     289   \n",
       "\n",
       "                                               tokenized  \n",
       "91648  How should I elicit prior distributions from e...  \n",
       "91649  In many different statistical methods there is...  \n",
       "91650  What are some valuable Statistical Analysis op...  \n",
       "91651  I have two groups of data . Each with a differ...  \n",
       "91652  The R - project R is valuable and significant ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How', 'should'),\n",
       " ('should', 'I'),\n",
       " ('I', 'elicit'),\n",
       " ('elicit', 'prior'),\n",
       " ('prior', 'distributions'),\n",
       " ('distributions', 'from'),\n",
       " ('from', 'experts'),\n",
       " ('experts', 'when'),\n",
       " ('when', 'fitting'),\n",
       " ('fitting', 'a')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_counts.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'I': 880,\n",
       "          'economists': 1,\n",
       "          'you': 15,\n",
       "          'one': 59,\n",
       "          'we': 40,\n",
       "          'such': 5,\n",
       "          'meta': 1,\n",
       "          'these': 9,\n",
       "          'it': 17,\n",
       "          'repeated': 1,\n",
       "          'the': 44,\n",
       "          '/': 3,\n",
       "          'decision': 2,\n",
       "          'choose': 1,\n",
       "          'i': 26,\n",
       "          'that': 7,\n",
       "          'she': 1,\n",
       "          'a': 4,\n",
       "          'criterion': 1,\n",
       "          'this': 25,\n",
       "          'my': 3,\n",
       "          \"'\": 1,\n",
       "          'proceed': 1,\n",
       "          'be': 5,\n",
       "          'x4': 1,\n",
       "          'tiny': 3,\n",
       "          'approach': 2,\n",
       "          'outliers': 6,\n",
       "          ',': 1,\n",
       "          'they': 3,\n",
       "          'data': 1,\n",
       "          'accuracy': 1,\n",
       "          'LSTM': 1,\n",
       "          '(': 1,\n",
       "          'do': 1,\n",
       "          'he': 2,\n",
       "          'people': 1,\n",
       "          'regression': 1,\n",
       "          'interpret': 1,\n",
       "          'change': 1,\n",
       "          'Bonferroni': 1,\n",
       "          'our': 1,\n",
       "          'an': 1,\n",
       "          'explain': 1,\n",
       "          'then': 1,\n",
       "          'imputation': 1,\n",
       "          'someone': 1,\n",
       "          'AIC': 1,\n",
       "          'Type': 1,\n",
       "          'mixed': 1,\n",
       "          '\"': 1,\n",
       "          'duplicate': 1}),\n",
       " Counter({'elicit': 1,\n",
       "          'use': 1458,\n",
       "          'model': 16,\n",
       "          'install': 1,\n",
       "          'search': 9,\n",
       "          'obtain': 5,\n",
       "          'be': 343,\n",
       "          'take': 125,\n",
       "          'then': 36,\n",
       "          'do': 881,\n",
       "          'transform': 43,\n",
       "          'employ': 5,\n",
       "          'inflate': 2,\n",
       "          'plot': 9,\n",
       "          'mathematically': 1,\n",
       "          'display': 2,\n",
       "          'spend': 3,\n",
       "          'worry': 13,\n",
       "          'find': 19,\n",
       "          'cut': 1,\n",
       "          'choose': 151,\n",
       "          'think': 24,\n",
       "          'calculate': 73,\n",
       "          'scan': 1,\n",
       "          'pick': 21,\n",
       "          'set': 28,\n",
       "          'analyze': 20,\n",
       "          'look': 106,\n",
       "          'continue': 12,\n",
       "          'multiply': 5,\n",
       "          'try': 83,\n",
       "          'treat': 53,\n",
       "          'make': 42,\n",
       "          'apply': 75,\n",
       "          'subtract': 3,\n",
       "          'analyse': 9,\n",
       "          'say': 41,\n",
       "          'delete': 23,\n",
       "          'care': 11,\n",
       "          'run': 66,\n",
       "          'define': 20,\n",
       "          'solve': 6,\n",
       "          'approach': 68,\n",
       "          'always': 18,\n",
       "          'account': 4,\n",
       "          'assign': 12,\n",
       "          'start': 40,\n",
       "          'give': 21,\n",
       "          'stick': 18,\n",
       "          'create': 20,\n",
       "          'only': 29,\n",
       "          'add': 33,\n",
       "          'determine': 29,\n",
       "          'build': 19,\n",
       "          'test': 45,\n",
       "          'just': 170,\n",
       "          'construct': 7,\n",
       "          'control': 5,\n",
       "          'submit': 2,\n",
       "          'read': 27,\n",
       "          'go': 152,\n",
       "          'interpret': 232,\n",
       "          'report': 48,\n",
       "          'consider': 123,\n",
       "          'not': 43,\n",
       "          'describe': 8,\n",
       "          'deal': 48,\n",
       "          'attribute': 1,\n",
       "          'rather': 18,\n",
       "          'include': 61,\n",
       "          'study': 10,\n",
       "          'decide': 21,\n",
       "          'maybe': 4,\n",
       "          'sort': 2,\n",
       "          'discretize': 1,\n",
       "          'fit': 16,\n",
       "          'adjust': 13,\n",
       "          'first': 26,\n",
       "          'proceed': 107,\n",
       "          'have': 60,\n",
       "          'design': 12,\n",
       "          'check': 30,\n",
       "          'chose': 4,\n",
       "          'conduct': 17,\n",
       "          'perform': 57,\n",
       "          'understand': 21,\n",
       "          'compare': 39,\n",
       "          'correct': 15,\n",
       "          'measure': 20,\n",
       "          'select': 23,\n",
       "          'handle': 35,\n",
       "          'combine': 15,\n",
       "          'propagate': 1,\n",
       "          'focus': 6,\n",
       "          'answer': 7,\n",
       "          'sample': 14,\n",
       "          'accept': 11,\n",
       "          'put': 33,\n",
       "          'conclude': 23,\n",
       "          'cluster': 2,\n",
       "          'group': 5,\n",
       "          'a': 1,\n",
       "          'restrict': 2,\n",
       "          'learn': 14,\n",
       "          'pre': 1,\n",
       "          'request': 1,\n",
       "          'need': 7,\n",
       "          'exclude': 17,\n",
       "          'ignore': 12,\n",
       "          'visualize': 3,\n",
       "          'feed': 1,\n",
       "          'expect': 45,\n",
       "          'leave': 14,\n",
       "          ':': 10,\n",
       "          'review': 1,\n",
       "          'trust': 35,\n",
       "          'collect': 9,\n",
       "          'instead': 17,\n",
       "          'tune': 5,\n",
       "          'estimate': 13,\n",
       "          'characterize': 2,\n",
       "          'weight': 5,\n",
       "          'guess': 4,\n",
       "          'call': 20,\n",
       "          'write': 25,\n",
       "          'get': 18,\n",
       "          'even': 10,\n",
       "          'note': 2,\n",
       "          'simply': 31,\n",
       "          'invert': 1,\n",
       "          'indicate': 3,\n",
       "          'believe': 9,\n",
       "          'arrange': 4,\n",
       "          'simulate': 4,\n",
       "          'reduce': 4,\n",
       "          'respond': 3,\n",
       "          ')': 3,\n",
       "          'forecast': 2,\n",
       "          'fill': 4,\n",
       "          'repeat': 7,\n",
       "          'compute': 31,\n",
       "          'manually': 3,\n",
       "          'remove': 37,\n",
       "          'impute': 8,\n",
       "          'wait': 2,\n",
       "          'open': 3,\n",
       "          'specify': 15,\n",
       "          'summarize': 3,\n",
       "          'avoid': 11,\n",
       "          'address': 5,\n",
       "          'also': 35,\n",
       "          'keep': 35,\n",
       "          'draw': 8,\n",
       "          'randomly': 6,\n",
       "          'prefer': 39,\n",
       "          'scale': 4,\n",
       "          'valuate': 1,\n",
       "          'recover': 1,\n",
       "          'reinit': 1,\n",
       "          'ask': 25,\n",
       "          'best': 12,\n",
       "          'identify': 3,\n",
       "          'simple': 1,\n",
       "          'smooth': 3,\n",
       "          'hire': 1,\n",
       "          'normalize': 19,\n",
       "          'present': 9,\n",
       "          'actually': 7,\n",
       "          'utilize': 3,\n",
       "          'consult': 1,\n",
       "          'let': 8,\n",
       "          'enforce': 1,\n",
       "          'classify': 3,\n",
       "          'type': 4,\n",
       "          ',': 9,\n",
       "          'purge': 1,\n",
       "          'repeatedly': 1,\n",
       "          'allow': 6,\n",
       "          'divide': 17,\n",
       "          'therefore': 3,\n",
       "          'pay': 7,\n",
       "          'organize': 3,\n",
       "          'self': 1,\n",
       "          'deliver': 1,\n",
       "          'replace': 8,\n",
       "          'discard': 8,\n",
       "          'work': 4,\n",
       "          'label': 1,\n",
       "          'move': 4,\n",
       "          'prepare': 2,\n",
       "          'input': 2,\n",
       "          'follow': 20,\n",
       "          'reflect': 1,\n",
       "          '...': 1,\n",
       "          'artificially': 1,\n",
       "          'market': 1,\n",
       "          'refer': 12,\n",
       "          'form': 1,\n",
       "          'remedy': 1,\n",
       "          'sum': 4,\n",
       "          'assess': 7,\n",
       "          'bid': 2,\n",
       "          'retry': 1,\n",
       "          'turn': 4,\n",
       "          'prove': 2,\n",
       "          'structure': 3,\n",
       "          'carry': 5,\n",
       "          'adhere': 1,\n",
       "          'adopt': 8,\n",
       "          'regard': 1,\n",
       "          'exploit': 1,\n",
       "          'at': 1,\n",
       "          'investigate': 7,\n",
       "          'explain': 6,\n",
       "          'still': 30,\n",
       "          'modify': 9,\n",
       "          'mention': 4,\n",
       "          'code': 4,\n",
       "          'aggregate': 3,\n",
       "          'pool': 3,\n",
       "          'assume': 20,\n",
       "          'somehow': 7,\n",
       "          'count': 5,\n",
       "          'standardize': 10,\n",
       "          'infer': 2,\n",
       "          'back': 1,\n",
       "          'rely': 9,\n",
       "          'average': 11,\n",
       "          'change': 31,\n",
       "          'pursue': 4,\n",
       "          'separate': 5,\n",
       "          'reason': 1,\n",
       "          '?': 14,\n",
       "          'play': 1,\n",
       "          'tackle': 1,\n",
       "          'bet': 1,\n",
       "          'enter': 4,\n",
       "          'rate': 1,\n",
       "          'bite': 1,\n",
       "          'now': 6,\n",
       "          'detect': 1,\n",
       "          'Correct': 1,\n",
       "          'state': 3,\n",
       "          'reject': 7,\n",
       "          '(': 8,\n",
       "          'recommend': 2,\n",
       "          'sell': 2,\n",
       "          'roll': 1,\n",
       "          'reverse': 1,\n",
       "          'loop': 1,\n",
       "          'over': 1,\n",
       "          'used': 12,\n",
       "          'see': 5,\n",
       "          'establish': 3,\n",
       "          'insert': 4,\n",
       "          'process': 2,\n",
       "          'efficiently': 2,\n",
       "          'generate': 6,\n",
       "          'research': 2,\n",
       "          'interpred': 2,\n",
       "          'abandon': 3,\n",
       "          'bin': 2,\n",
       "          'correctly': 3,\n",
       "          'syntax': 1,\n",
       "          'predict': 4,\n",
       "          'completely': 2,\n",
       "          'download': 1,\n",
       "          'further': 1,\n",
       "          'un': 1,\n",
       "          'formalize': 2,\n",
       "          'stop': 11,\n",
       "          'discern': 1,\n",
       "          'break': 3,\n",
       "          'optimally': 1,\n",
       "          'blindly': 1,\n",
       "          'know': 10,\n",
       "          'for': 5,\n",
       "          're': 10,\n",
       "          'increment': 1,\n",
       "          'attempt': 1,\n",
       "          'refresh': 1,\n",
       "          'come': 3,\n",
       "          'train': 8,\n",
       "          'attach': 1,\n",
       "          'dive': 2,\n",
       "          'explore': 2,\n",
       "          'increase': 6,\n",
       "          'drop': 9,\n",
       "          'omit': 2,\n",
       "          'really': 6,\n",
       "          '\"': 9,\n",
       "          'subset': 1,\n",
       "          'improve': 3,\n",
       "          'necessarily': 3,\n",
       "          'name': 1,\n",
       "          'cache': 1,\n",
       "          'built': 2,\n",
       "          'phrase': 1,\n",
       "          'nail': 1,\n",
       "          'deduce': 2,\n",
       "          'fix': 6,\n",
       "          'upsample': 1,\n",
       "          'implement': 9,\n",
       "          'watch': 1,\n",
       "          'split': 7,\n",
       "          'quantify': 1,\n",
       "          'suppose': 1,\n",
       "          'delve': 1,\n",
       "          'integrate': 6,\n",
       "          'properly': 3,\n",
       "          'segregate': 1,\n",
       "          'concern': 1,\n",
       "          'pass': 5,\n",
       "          'filter': 3,\n",
       "          'manupliate': 1,\n",
       "          'incorporate': 6,\n",
       "          'formulate': 8,\n",
       "          'place': 3,\n",
       "          'encode': 3,\n",
       "          'resample': 3,\n",
       "          'begin': 4,\n",
       "          'progress': 2,\n",
       "          'interview': 1,\n",
       "          'extract': 3,\n",
       "          'represent': 8,\n",
       "          'invest': 3,\n",
       "          'aim': 2,\n",
       "          'preprocess': 3,\n",
       "          'distribute': 2,\n",
       "          'stratify': 1,\n",
       "          'separately': 1,\n",
       "          'dummy': 2,\n",
       "          '/': 3,\n",
       "          'interpreted': 3,\n",
       "          'log': 4,\n",
       "          'scal': 1,\n",
       "          'differentiate': 2,\n",
       "          'reasonably': 1,\n",
       "          'resolve': 1,\n",
       "          'introduce': 3,\n",
       "          'procedd': 1,\n",
       "          'dismiss': 1,\n",
       "          'configure': 2,\n",
       "          'plan': 1,\n",
       "          'throw': 2,\n",
       "          'show': 2,\n",
       "          'minimize': 5,\n",
       "          'provide': 12,\n",
       "          'strip': 1,\n",
       "          'annualize': 1,\n",
       "          'bother': 1,\n",
       "          'trim': 1,\n",
       "          'verify': 1,\n",
       "          'tread': 1,\n",
       "          'analysis': 1,\n",
       "          'obfuscate': 1,\n",
       "          'merge': 4,\n",
       "          'center': 2,\n",
       "          'de': 1,\n",
       "          'evaluate': 7,\n",
       "          'log2': 1,\n",
       "          'categorize': 1,\n",
       "          'exactly': 3,\n",
       "          'debug': 1,\n",
       "          'forget': 2,\n",
       "          'live': 1,\n",
       "          'update': 4,\n",
       "          'resort': 1,\n",
       "          'specifiy': 1,\n",
       "          'initialize': 4,\n",
       "          'convince': 1,\n",
       "          'express': 3,\n",
       "          'judge': 1,\n",
       "          'convert': 10,\n",
       "          'confirm': 2,\n",
       "          'suspect': 1,\n",
       "          'one': 1,\n",
       "          'reshuffle': 1,\n",
       "          'sit': 1,\n",
       "          'evalute': 1,\n",
       "          'consume': 1,\n",
       "          'parallelize': 1,\n",
       "          'am': 1,\n",
       "          'by': 1,\n",
       "          'analytically': 1,\n",
       "          'manage': 1,\n",
       "          'examine': 3,\n",
       "          'generally': 2,\n",
       "          'anyway': 1,\n",
       "          'sub': 1,\n",
       "          'sum(compare.data[,\"predict': 1,\n",
       "          'literally': 1,\n",
       "          'interprete': 1,\n",
       "          'NOT': 1,\n",
       "          'memorize': 1,\n",
       "          'rule': 1,\n",
       "          'in': 1,\n",
       "          'recognize': 3,\n",
       "          'use-': 1,\n",
       "          'oversample': 1,\n",
       "          'force': 3,\n",
       "          'perhaps': 2,\n",
       "          'simplify': 1,\n",
       "          'statistically': 1,\n",
       "          'post': 14,\n",
       "          'seek': 2,\n",
       "          'derive': 2,\n",
       "          'link': 1,\n",
       "          'conceive': 1,\n",
       "          'mark': 1,\n",
       "          'factor': 2,\n",
       "          'notice': 1,\n",
       "          'operate': 1,\n",
       "          'cater': 1,\n",
       "          'procede': 2,\n",
       "          'graph': 2,\n",
       "          'rank': 1,\n",
       "          'appropriately': 1,\n",
       "          'bootstrap': 2,\n",
       "          'recalculate': 1,\n",
       "          'preprocessing': 1,\n",
       "          'space': 2,\n",
       "          'match': 1,\n",
       "          'allocate': 2,\n",
       "          'shape': 1,\n",
       "          'user': 1,\n",
       "          'mean': 1,\n",
       "          'join': 1,\n",
       "          'ban': 1,\n",
       "          'drop?70': 1,\n",
       "          'concatenate': 1,\n",
       "          'monitor': 1,\n",
       "          'quote': 1,\n",
       "          'fine': 1,\n",
       "          'act': 3,\n",
       "          'revise': 2,\n",
       "          'training': 1,\n",
       "          'plant': 1,\n",
       "          'rescale': 2,\n",
       "          'disregard': 1,\n",
       "          'instrument': 1,\n",
       "          'difference': 2,\n",
       "          'decrease': 1,\n",
       "          'figure': 3,\n",
       "          'react': 1,\n",
       "          'record': 1,\n",
       "          'calcualte': 1,\n",
       "          'refit': 1,\n",
       "          'prune': 1,\n",
       "          'push': 1,\n",
       "          'systematically': 1,\n",
       "          'translate': 1,\n",
       "          'radnomly': 1,\n",
       "          'reliably': 1,\n",
       "          'format': 2,\n",
       "          'optimize': 1,\n",
       "          'normalised': 1,\n",
       "          'amend': 1,\n",
       "          'switch': 2,\n",
       "          'minimise': 1,\n",
       "          'stipulate': 1,\n",
       "          'standardise': 1,\n",
       "          'iterate': 1,\n",
       "          'standardized': 1,\n",
       "          'tell': 2,\n",
       "          'balance': 2,\n",
       "          'opt': 2,\n",
       "          'underfit': 1,\n",
       "          'mistrust': 1,\n",
       "          'i': 1,\n",
       "          'picture': 1,\n",
       "          'differenciate': 2,\n",
       "          'overcome': 1,\n",
       "          'as': 1,\n",
       "          'tidy': 1,\n",
       "          'done': 1,\n",
       "          'connect': 1,\n",
       "          'stay': 2,\n",
       "          'correlate': 2,\n",
       "          'buy': 4,\n",
       "          'justify': 2,\n",
       "          'number': 1,\n",
       "          'weigh': 1,\n",
       "          'tab': 2,\n",
       "          'end': 1,\n",
       "          'return': 3,\n",
       "          'reformulate': 1,\n",
       "          'analise': 1,\n",
       "          'retain': 1,\n",
       "          'compile': 1,\n",
       "          'shuffle': 1,\n",
       "          'better': 2,\n",
       "          'release': 1,\n",
       "          'roughly': 1,\n",
       "          'recode': 2,\n",
       "          'concentrate': 1,\n",
       "          'extend': 1,\n",
       "          'cover': 1,\n",
       "          'map': 1,\n",
       "          'rephrase': 3,\n",
       "          'from': 1,\n",
       "          'or': 2,\n",
       "          'edit': 10,\n",
       "          'discuss': 1,\n",
       "          'condition': 1,\n",
       "          'target': 1,\n",
       "          'relax': 1,\n",
       "          'setup': 1,\n",
       "          'was': 1,\n",
       "          'limit': 1,\n",
       "          'want': 1,\n",
       "          'validate': 1,\n",
       "          'produce': 1,\n",
       "          'report?': 1,\n",
       "          'penalize': 1,\n",
       "          'fitt': 1,\n",
       "          'rewrite': 1,\n",
       "          'violate': 1,\n",
       "          'bucket': 1,\n",
       "          'acquire': 2,\n",
       "          'proceed?Can': 1,\n",
       "          'suggest': 2,\n",
       "          'redo': 1,\n",
       "          'thus': 1,\n",
       "          'skip': 2,\n",
       "          'strive': 2,\n",
       "          'clarify': 3,\n",
       "          'designate': 1,\n",
       "          'nest': 1,\n",
       "          'cross': 1,\n",
       "          'feel': 1,\n",
       "          'interact': 1,\n",
       "          'plug': 1,\n",
       "          'choose??.Either': 1,\n",
       "          'google': 1,\n",
       "          'impose': 1,\n",
       "          'marginalize': 1,\n",
       "          'wear': 1,\n",
       "          'share': 1,\n",
       "          'undertake': 1,\n",
       "          'visualise': 1,\n",
       "          'send': 1,\n",
       "          'denormolize': 1,\n",
       "          'base': 1,\n",
       "          'close': 2,\n",
       "          'bootrstap': 1,\n",
       "          'collate': 1,\n",
       "          'offer': 1,\n",
       "          'den': 1,\n",
       "          'arbitrarily': 1,\n",
       "          'arbitrate': 1,\n",
       "          'elaborate': 2,\n",
       "          'program': 1,\n",
       "          'loose': 1,\n",
       "          'expand': 2,\n",
       "          'communicate': 1,\n",
       "          'undo': 1,\n",
       "          'vary': 1,\n",
       "          'ping': 1,\n",
       "          'directly': 2,\n",
       "          'presume': 1,\n",
       "          'inspect': 1,\n",
       "          'ideally': 1,\n",
       "          'partition': 1,\n",
       "          'imagine': 1,\n",
       "          'atleast': 1,\n",
       "          'chalk': 1,\n",
       "          'specifically': 1,\n",
       "          'do?This': 1,\n",
       "          'covert': 1,\n",
       "          'import': 1,\n",
       "          'relate': 2,\n",
       "          'deadl': 1,\n",
       "          'discount': 1,\n",
       "          'cite': 2,\n",
       "          'erase': 1,\n",
       "          'slice': 1,\n",
       "          'lose': 1,\n",
       "          'fist': 1,\n",
       "          'tweak': 1,\n",
       "          'rethink': 1,\n",
       "          'resume': 1,\n",
       "          'batch': 2,\n",
       "          'never': 1,\n",
       "          'talk': 1,\n",
       "          'realise': 1,\n",
       "          'reengage': 1,\n",
       "          'supply': 1,\n",
       "          '[': 1,\n",
       "          'reshape': 1,\n",
       "          'heed': 1}),\n",
       " Counter({'prior': 1}),\n",
       " Counter({'distributions': 1}),\n",
       " Counter({'from': 2,\n",
       "          'can': 4,\n",
       "          'for': 24,\n",
       "          'in': 7,\n",
       "          'that': 7,\n",
       "          \"'\": 1,\n",
       "          'together': 1,\n",
       "          'on': 7,\n",
       "          'look': 1,\n",
       "          ')': 4,\n",
       "          '?': 7,\n",
       "          ',': 27,\n",
       "          ':': 2,\n",
       "          '.': 35,\n",
       "          'be': 1,\n",
       "          'there': 1,\n",
       "          'to': 6,\n",
       "          'are': 12,\n",
       "          'would': 1,\n",
       "          'whereas': 1,\n",
       "          'should': 1,\n",
       "          'do': 1,\n",
       "          '(': 4,\n",
       "          'is': 1,\n",
       "          'then': 1,\n",
       "          'but': 1,\n",
       "          'of': 3,\n",
       "          'just': 2,\n",
       "          'were': 3,\n",
       "          'with': 3,\n",
       "          'and': 4,\n",
       "          'over': 2,\n",
       "          'by': 3,\n",
       "          'associated': 1,\n",
       "          'have': 2,\n",
       "          'I': 2,\n",
       "          'used': 1,\n",
       "          'according': 1,\n",
       "          '\"': 2,\n",
       "          'themselves': 1,\n",
       "          'at': 2,\n",
       "          'given': 1,\n",
       "          'chosen': 1,\n",
       "          'called': 1,\n",
       "          'which': 2,\n",
       "          'you': 2,\n",
       "          'onto': 1,\n",
       "          'defined': 1,\n",
       "          'and/or': 1,\n",
       "          'we': 1,\n",
       "          'when': 1}),\n",
       " Counter({'experts': 1,\n",
       "          'D': 1,\n",
       "          'two': 2,\n",
       "          'scratch': 1,\n",
       "          'which': 18,\n",
       "          'a': 11,\n",
       "          'the': 23,\n",
       "          'normal': 1,\n",
       "          'what': 1,\n",
       "          'histograms': 2,\n",
       "          'one': 2,\n",
       "          'some': 3,\n",
       "          'both': 1,\n",
       "          'data': 2,\n",
       "          'separate': 1,\n",
       "          'first': 1,\n",
       "          'scattered': 1,\n",
       "          'to': 1,\n",
       "          'each': 1,\n",
       "          'statistical': 1,\n",
       "          'this': 4,\n",
       "          'other': 1,\n",
       "          'them': 2,\n",
       "          'A': 1,\n",
       "          'copulas': 2,\n",
       "          'left': 1,\n",
       "          'bootstrapping': 1,\n",
       "          'large': 1,\n",
       "          'your': 2,\n",
       "          'Haynes': 1,\n",
       "          'knowledge': 1,\n",
       "          'that': 1,\n",
       "          'ordinal': 1,\n",
       "          'resulting': 1,\n",
       "          'their': 2,\n",
       "          'random': 1,\n",
       "          'uniform': 1,\n",
       "          'simulation': 1,\n",
       "          'these': 1,\n",
       "          'sampled': 1,\n",
       "          'clients': 1,\n",
       "          'store': 1,\n",
       "          'linear': 1,\n",
       "          'my': 1,\n",
       "          'marginals': 1,\n",
       "          'scipy': 1,\n",
       "          'joint': 1,\n",
       "          'different': 1,\n",
       "          'independent': 1}),\n",
       " Counter({'when': 1,\n",
       "          'What': 1,\n",
       "          'whereas': 1,\n",
       "          'in': 5,\n",
       "          'if': 1,\n",
       "          '(': 2,\n",
       "          ',': 3,\n",
       "          'here': 1,\n",
       "          '.': 8,\n",
       "          'on': 1,\n",
       "          ':)': 1,\n",
       "          'using': 1,\n",
       "          'calibrated': 1,\n",
       "          'as': 1,\n",
       "          'like': 3,\n",
       "          'to': 1,\n",
       "          'and': 2}),\n",
       " Counter({'fitting': 1}),\n",
       " Counter({'a': 72,\n",
       "          '(': 1,\n",
       "          'other': 1,\n",
       "          'mixed': 2,\n",
       "          'GAM': 1,\n",
       "          'with': 3,\n",
       "          'the': 81,\n",
       "          'higher': 1,\n",
       "          '.': 6,\n",
       "          'an': 8,\n",
       "          'that': 2,\n",
       "          'vectors': 1,\n",
       "          'ARIMA': 1,\n",
       "          'data': 3,\n",
       "          'SEM': 1,\n",
       "          'model': 4,\n",
       "          'linear': 2,\n",
       "          'interactions': 3,\n",
       "          'to': 1,\n",
       "          ',': 1,\n",
       "          'GLMMs': 1,\n",
       "          'and': 2,\n",
       "          'your': 5,\n",
       "          'growth': 1,\n",
       "          'natural': 1,\n",
       "          'regressions': 1,\n",
       "          'ML': 1,\n",
       "          'GAMs': 2,\n",
       "          'polynomials': 1,\n",
       "          'coxph': 1,\n",
       "          'LMMs': 1,\n",
       "          'lasso': 1,\n",
       "          'on': 2,\n",
       "          'multivariate': 1,\n",
       "          'models': 4,\n",
       "          'no': 1,\n",
       "          'machine': 1,\n",
       "          'multilevel': 1,\n",
       "          'this': 1,\n",
       "          'regularized': 1,\n",
       "          'lighter': 1,\n",
       "          'for': 1,\n",
       "          'truncated': 1,\n",
       "          'without': 1,\n",
       "          'polynomial': 1,\n",
       "          'their': 1,\n",
       "          'these': 2,\n",
       "          ':': 1,\n",
       "          'Poisson': 1,\n",
       "          \"'\": 1,\n",
       "          'even': 1,\n",
       "          'GLMs': 1,\n",
       "          'AR(1': 1,\n",
       "          'at': 1,\n",
       "          'RF': 1,\n",
       "          'several': 1,\n",
       "          'smooth': 1,\n",
       "          '?': 1,\n",
       "          'two': 1,\n",
       "          'neural': 1,\n",
       "          '...': 1,\n",
       "          'any': 1,\n",
       "          'power': 1,\n",
       "          'Gaussian': 1,\n",
       "          'autoregressive': 1,\n",
       "          'lognormal': 1,\n",
       "          'may': 1}),\n",
       " Counter({'Bayesian': 8,\n",
       "          'Pareto': 2,\n",
       "          'large': 4,\n",
       "          'GLM': 28,\n",
       "          'model': 305,\n",
       "          'distribution': 49,\n",
       "          'logistic': 71,\n",
       "          'small': 1,\n",
       "          'seasonal': 2,\n",
       "          'straight': 28,\n",
       "          '4': 2,\n",
       "          'simple': 25,\n",
       "          'binomial': 11,\n",
       "          'Tweedie': 1,\n",
       "          'glm': 8,\n",
       "          'new': 11,\n",
       "          'single': 22,\n",
       "          'specific': 2,\n",
       "          'reduced': 1,\n",
       "          'conditional': 3,\n",
       "          'nested': 5,\n",
       "          'multilevel': 5,\n",
       "          'priori': 1,\n",
       "          'Binomial': 1,\n",
       "          'GLMM': 5,\n",
       "          'more': 4,\n",
       "          'regression': 83,\n",
       "          'spline': 7,\n",
       "          'compound': 2,\n",
       "          'curve': 31,\n",
       "          'flexible': 2,\n",
       "          'mixture': 20,\n",
       "          'univariate': 4,\n",
       "          'cubic': 9,\n",
       "          'response': 2,\n",
       "          'hyperprior': 1,\n",
       "          'generalized': 19,\n",
       "          'mixed': 37,\n",
       "          'marginal': 1,\n",
       "          '3-level': 1,\n",
       "          'natural': 2,\n",
       "          'multiple': 11,\n",
       "          'neural': 12,\n",
       "          'range': 2,\n",
       "          'variogram': 2,\n",
       "          'copula': 3,\n",
       "          'particular': 10,\n",
       "          'loess': 4,\n",
       "          'line': 51,\n",
       "          'polynomial': 23,\n",
       "          'linear': 150,\n",
       "          'normal': 29,\n",
       "          'statistical': 1,\n",
       "          'quasi': 1,\n",
       "          'second': 4,\n",
       "          'hierarchical': 4,\n",
       "          'non': 18,\n",
       "          'CFA': 1,\n",
       "          '2d': 2,\n",
       "          'weighted': 3,\n",
       "          'negative': 11,\n",
       "          'least': 1,\n",
       "          'sigmoid': 4,\n",
       "          '3-gaussian': 1,\n",
       "          'random': 18,\n",
       "          'general': 5,\n",
       "          'regular': 1,\n",
       "          'staionary': 1,\n",
       "          'gam': 5,\n",
       "          'smoother': 2,\n",
       "          'type': 1,\n",
       "          'Cox': 11,\n",
       "          'separate': 7,\n",
       "          'tree': 6,\n",
       "          'panel': 2,\n",
       "          'harmonic': 2,\n",
       "          'quadratic': 16,\n",
       "          'mean': 3,\n",
       "          'horizontal': 2,\n",
       "          'Polynomial': 1,\n",
       "          'svm': 1,\n",
       "          'GEE': 5,\n",
       "          'Gaussian': 20,\n",
       "          'proportional': 2,\n",
       "          'decision': 5,\n",
       "          'GMM': 7,\n",
       "          'Poisson': 26,\n",
       "          'fourier': 1,\n",
       "          'constant': 4,\n",
       "          'continuous': 5,\n",
       "          'simpler': 2,\n",
       "          'power': 17,\n",
       "          'time': 10,\n",
       "          'function': 17,\n",
       "          'nonparametric': 1,\n",
       "          'higher': 2,\n",
       "          'Random': 4,\n",
       "          'spatial': 1,\n",
       "          'gaussian': 9,\n",
       "          'good': 1,\n",
       "          'probability': 5,\n",
       "          'GAM': 12,\n",
       "          'Normal': 2,\n",
       "          'posterior': 1,\n",
       "          'GARCH': 13,\n",
       "          'parameter': 2,\n",
       "          'MA': 2,\n",
       "          'meta': 1,\n",
       "          'kind': 2,\n",
       "          'heteroschedastic': 1,\n",
       "          'parametric': 8,\n",
       "          'dummy': 3,\n",
       "          'Weibull': 9,\n",
       "          'smooth': 4,\n",
       "          'count': 3,\n",
       "          'maximum': 1,\n",
       "          'completely': 3,\n",
       "          'value': 1,\n",
       "          'PCA': 2,\n",
       "          'full': 7,\n",
       "          'lm': 1,\n",
       "          'hyperplane': 2,\n",
       "          'multinomial': 7,\n",
       "          'multivariate': 8,\n",
       "          '1-dimensional': 1,\n",
       "          '100-order': 1,\n",
       "          'stationary': 2,\n",
       "          'classification': 1,\n",
       "          'set': 6,\n",
       "          'lasso': 2,\n",
       "          'Student': 1,\n",
       "          'Quadric': 1,\n",
       "          'penalized': 1,\n",
       "          'relationship': 4,\n",
       "          'basic': 3,\n",
       "          'quantile': 2,\n",
       "          'ridge': 1,\n",
       "          '\"': 9,\n",
       "          'Low': 1,\n",
       "          '(': 6,\n",
       "          'well': 2,\n",
       "          '200': 1,\n",
       "          'super': 1,\n",
       "          'plane': 8,\n",
       "          'glm.nb': 1,\n",
       "          'KDE': 3,\n",
       "          'piecewise': 6,\n",
       "          'global': 1,\n",
       "          'truncated': 3,\n",
       "          'cox': 1,\n",
       "          'nonlinear': 7,\n",
       "          'lognormal': 1,\n",
       "          'pre': 2,\n",
       "          'memory': 1,\n",
       "          'gamma': 13,\n",
       "          'suitable': 3,\n",
       "          'trial': 1,\n",
       "          'signal': 1,\n",
       "          'survival': 3,\n",
       "          'common': 1,\n",
       "          'uni': 1,\n",
       "          'multi': 3,\n",
       "          'categorical': 1,\n",
       "          'Gumbel': 1,\n",
       "          'Lasso': 3,\n",
       "          'triangle': 1,\n",
       "          'timeseries': 1,\n",
       "          '1st': 1,\n",
       "          'KNN': 1,\n",
       "          'parabola': 3,\n",
       "          'different': 10,\n",
       "          'collection': 1,\n",
       "          'bell': 2,\n",
       "          'series': 3,\n",
       "          'standard': 7,\n",
       "          'B': 1,\n",
       "          'cumulative': 1,\n",
       "          'binary': 4,\n",
       "          'ARCH': 1,\n",
       "          'VAR': 8,\n",
       "          'regularised': 1,\n",
       "          'quite': 1,\n",
       "          'latent': 1,\n",
       "          'number': 4,\n",
       "          'rather': 1,\n",
       "          'VECM': 3,\n",
       "          'GEV': 3,\n",
       "          'logarithmic': 2,\n",
       "          'log': 4,\n",
       "          'family': 1,\n",
       "          'fixed': 7,\n",
       "          'Taylor': 1,\n",
       "          'structural': 5,\n",
       "          'GAMM': 1,\n",
       "          'trendline': 1,\n",
       "          'functional': 4,\n",
       "          'trend': 9,\n",
       "          'topic': 1,\n",
       "          'two': 2,\n",
       "          'one': 1,\n",
       "          'term': 2,\n",
       "          'periodic': 2,\n",
       "          'GP': 5,\n",
       "          'fully': 1,\n",
       "          'bounding': 1,\n",
       "          'SEM': 1,\n",
       "          'LLS': 1,\n",
       "          'ARIMA': 2,\n",
       "          'glmer.nb': 1,\n",
       "          '4-deg': 1,\n",
       "          'population': 2,\n",
       "          'long': 1,\n",
       "          'poisson': 5,\n",
       "          'Logistic': 1,\n",
       "          'the': 1,\n",
       "          'third': 1,\n",
       "          'tenth': 1,\n",
       "          'smoothing': 2,\n",
       "          'LOESS': 2,\n",
       "          'Ridge': 1,\n",
       "          'Neural': 1,\n",
       "          'bunch': 5,\n",
       "          'sinusoidal': 1,\n",
       "          't': 5,\n",
       "          '1-component': 1,\n",
       "          'bivariate': 5,\n",
       "          'bulge': 1,\n",
       "          'SVM': 1,\n",
       "          'given': 2,\n",
       "          'confidence': 1,\n",
       "          'few': 4,\n",
       "          'Case': 1,\n",
       "          'weibull': 2,\n",
       "          'dynamic': 1,\n",
       "          'rating': 1,\n",
       "          'surface': 1,\n",
       "          'AR(p': 1,\n",
       "          'three': 7,\n",
       "          'Weinbull': 1,\n",
       "          'lot': 2,\n",
       "          'glmer': 1,\n",
       "          'regularized': 3,\n",
       "          'Copula': 1,\n",
       "          'geeglm': 1,\n",
       "          '9th': 1,\n",
       "          'sliding': 1,\n",
       "          'beta': 10,\n",
       "          \"'\": 1,\n",
       "          'Beta': 1,\n",
       "          '@#$^': 1,\n",
       "          'complex': 1,\n",
       "          '2-D': 1,\n",
       "          '2nd': 1,\n",
       "          'gompertz': 1,\n",
       "          'classical': 2,\n",
       "          'Cauchy': 1,\n",
       "          'Lorentzian': 1,\n",
       "          'Bernoulli': 1,\n",
       "          'spike': 1,\n",
       "          'Tobit': 1,\n",
       "          'boosted': 1,\n",
       "          'Generalized': 1,\n",
       "          'degree': 2,\n",
       "          'many': 1,\n",
       "          'bayesian': 1,\n",
       "          'Propensity': 1,\n",
       "          '`': 3,\n",
       "          'logit': 4,\n",
       "          'classifier': 2,\n",
       "          'preidctive': 1,\n",
       "          'predictive': 2,\n",
       "          'ZIP': 1,\n",
       "          '20': 1,\n",
       "          '$': 2,\n",
       "          'whole': 1,\n",
       "          'pair': 1,\n",
       "          'GARCH(1,1': 2,\n",
       "          'generalised': 1,\n",
       "          'prior': 1,\n",
       "          'misspecified': 2,\n",
       "          'multivariable': 1,\n",
       "          'sinusoid': 1,\n",
       "          'Linear': 2,\n",
       "          'hierarchal': 1,\n",
       "          'GBM': 1,\n",
       "          'physical': 1,\n",
       "          'covariance': 3,\n",
       "          'very': 3,\n",
       "          '6-parameter': 1,\n",
       "          'sine': 4,\n",
       "          'joint': 3,\n",
       "          'prediction': 1,\n",
       "          'to': 1,\n",
       "          'a': 1,\n",
       "          'zeta': 1,\n",
       "          'high': 3,\n",
       "          'correlated': 1,\n",
       "          'probabilistic': 2,\n",
       "          'HMM': 1,\n",
       "          'Gamma': 4,\n",
       "          'sklearn.ensemble.RandomForestRegressor': 1,\n",
       "          'standardized': 1,\n",
       "          'couple': 3,\n",
       "          'final': 1,\n",
       "          '2-factor': 1,\n",
       "          'restricted': 2,\n",
       "          'known': 1,\n",
       "          'lowess': 2,\n",
       "          'RF': 2,\n",
       "          '4-level': 1,\n",
       "          'continuously': 1,\n",
       "          'stright': 1,\n",
       "          'Guassian': 1,\n",
       "          'causal': 1,\n",
       "          'quarterly': 1,\n",
       "          'Pearson': 1,\n",
       "          'fifth': 1,\n",
       "          'monotone': 1,\n",
       "          '_': 1,\n",
       "          'Fine': 1,\n",
       "          'garch': 1,\n",
       "          'frailty': 1,\n",
       "          '[': 1,\n",
       "          'piece': 2,\n",
       "          'Burr': 1,\n",
       "          'location': 1,\n",
       "          'density': 2,\n",
       "          'candidate': 1,\n",
       "          'subset': 1,\n",
       "          'noncollapsible': 1,\n",
       "          'sigmoidal': 3,\n",
       "          'lme': 1,\n",
       "          'CDF': 1,\n",
       "          'deterministic': 2,\n",
       "          'deep': 1,\n",
       "          '*': 4,\n",
       "          'fairly': 1,\n",
       "          'dataset': 2,\n",
       "          'TS': 1,\n",
       "          'data': 3,\n",
       "          'double': 3,\n",
       "          'best': 1,\n",
       "          'bernoulli': 1,\n",
       "          'Binomoal': 1,\n",
       "          'zero': 2,\n",
       "          'negbinom': 1,\n",
       "          'plain': 1,\n",
       "          'repeated': 1,\n",
       "          'arma': 1,\n",
       "          'circle': 1,\n",
       "          '10th': 1,\n",
       "          'LASSO': 1,\n",
       "          'similar': 1,\n",
       "          'zinb': 1,\n",
       "          'robust': 1,\n",
       "          'maximal': 1,\n",
       "          'substantively': 1,\n",
       "          'bogus': 1,\n",
       "          'mechanistic': 1,\n",
       "          'stable': 1,\n",
       "          'skewed': 1,\n",
       "          'unified': 1,\n",
       "          'low': 1,\n",
       "          'Markov': 1,\n",
       "          'Cochran': 1,\n",
       "          'residual': 1,\n",
       "          'logical': 1,\n",
       "          'bad': 1,\n",
       "          'quasipoisson': 1,\n",
       "          'Kaplan': 1,\n",
       "          '“': 1,\n",
       "          'much': 1,\n",
       "          'SDE': 1,\n",
       "          'OLS': 1,\n",
       "          'true': 1,\n",
       "          'typical': 1,\n",
       "          'combination': 1,\n",
       "          'LS': 1})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I need to see some counts over 1 or I'm going to expect that I have this wrong. \n",
    "list(raw_counts.values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'this': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91649     In many different statistical methods there is...\n",
       "91662     Two projects spring to mind : Bugs - taking ( ...\n",
       "91724     I had a plan of learning R in the near future ...\n",
       "91748     Let us say a man rolls a six sided die and it ...\n",
       "91779     This is one I 've used successfully : I just s...\n",
       "                                ...                        \n",
       "808933    I do n't know * anything * about your data oth...\n",
       "808983    Thank you for the answer . yes your understand...\n",
       "809075    @whuber I know how to compute p value from sta...\n",
       "809076    @ŁukaszGrad I know S1 ^ 2&S2 ^ 2 shown in abov...\n",
       "809081    @whuber Now I understand your previous comment...\n",
       "Name: tokenized, Length: 14388, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This shows that my bigram counter is broken. \n",
    "\n",
    "#seems like something I'd find.\n",
    "bg  = ('I', 'know')\n",
    "jbg = ' '.join(bg)\n",
    "print(raw_counts[bg])\n",
    "\n",
    "#check in the training set\n",
    "found = train_df['tokenized'].str.contains(jbg)\n",
    "train_df[found]['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How should I elicit prior distributions from experts when fitting a Bayesian model ?\n"
     ]
    }
   ],
   "source": [
    "# does ngram splitting include the padding? \n",
    "i = 0\n",
    "s = train_df.iloc[i]['tokenized']\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', '<s>', 'How'), ('<s>', 'How', 'should'), ('How', 'should', 'I'), ('should', 'I', 'elicit'), ('I', 'elicit', 'prior'), ('elicit', 'prior', 'distributions'), ('prior', 'distributions', 'from'), ('distributions', 'from', 'experts'), ('from', 'experts', 'when'), ('experts', 'when', 'fitting'), ('when', 'fitting', 'a'), ('fitting', 'a', 'Bayesian'), ('a', 'Bayesian', 'model'), ('Bayesian', 'model', '?'), ('model', '?', '</s>'), ('?', '</s>', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "#Note that it starts with two padding characters. \n",
    "x = list(ngrams(s.split(' '), 3,  pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How', 'should', 'I'), ('should', 'I', 'elicit'), ('I', 'elicit', 'prior'), ('elicit', 'prior', 'distributions'), ('prior', 'distributions', 'from'), ('distributions', 'from', 'experts'), ('from', 'experts', 'when'), ('experts', 'when', 'fitting'), ('when', 'fitting', 'a'), ('fitting', 'a', 'Bayesian'), ('a', 'Bayesian', 'model'), ('Bayesian', 'model', '?')]\n"
     ]
    }
   ],
   "source": [
    "x = list(ngrams(s.split(' '), 3,  pad_left=False, pad_right=False, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
