{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "This time we can use a larger chunk of the original dataset without running into so many memory and runtime problems.\n",
    "\n",
    "1. Explore and analyze the set of unique characters present in the dataset.\n",
    "1. Subsample the dataset to take into account the titles only.\n",
    "1. Implement the tokenization of the dataset and transform the tokens into AllenNLP Instances.\n",
    "1. Design a RNN with AllenNLP that includes:\n",
    "   1. an embedding of the tokens,\n",
    "   1. a seq2seq LSTM layer,\n",
    "   1. a feed forward layer that outputs probability distribution of the characters\n",
    "1. Train the model.\n",
    "1. Evaluate the model by calculating the loss of some sentences and by generating text.\n",
    "\n",
    "### Bonus\n",
    "\n",
    "Can you swap a Transformer-based model in for an LSTM? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* [AllenNLP the hard way: Building a Baseline Model](https://jbarrow.ai/allennlp-the-hard-way-2/)\n",
    "* [Sequential Labeling and Language Modeling (Chapter 5 - Covers allenNLP and Char RNN LM)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-5/v-5/35)\n",
    "* [Atentions and the Transformer (Chapter 8)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-8/v-5/)\n",
    "* [Sequence to Sequence Models (Chapter 6)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-6/v-5/)\n",
    "* [An In Depth AllenNLP tutorial (Basics to BERT)](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)\n",
    "* [Official AllenNLP tutorial](https://allennlp.org/tutorials)\n",
    "* [AllenNLP Guide - Reading Data](https://guide.allennlp.org/reading-data)\n",
    "* [AllenNLP Simple Language Model DatasetReader](https://github.com/allenai/allennlp-models/blob/master/allennlp_models/lm/dataset_readers/simple_language_modeling.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK_FILE= 'stackexchange_tokenized.csv'\n",
    "df = pd.read_csv(f'data/{TOK_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Unnamed: 0  post_id  parent_id  comment_id  \\\n0           0        1        NaN         NaN   \n1           1        2        NaN         NaN   \n2           2        3        NaN         NaN   \n3           3        4        NaN         NaN   \n4           4        6        NaN         NaN   \n\n                                                text category  length  \\\n0                      Eliciting priors from experts    title      29   \n1                                 What is normality?    title      18   \n2  What are some valuable Statistical Analysis op...    title      65   \n3  Assessing the significance of differences in d...    title      58   \n4  The Two Cultures: statistics vs. machine learn...    title      50   \n\n                                           tokenized  \n0                      Eliciting priors from experts  \n1                                What is normality ?  \n2  What are some valuable Statistical Analysis op...  \n3  Assessing the significance of differences in d...  \n4  The Two Cultures : statistics vs. machine lear...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>post_id</th>\n      <th>parent_id</th>\n      <th>comment_id</th>\n      <th>text</th>\n      <th>category</th>\n      <th>length</th>\n      <th>tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Eliciting priors from experts</td>\n      <td>title</td>\n      <td>29</td>\n      <td>Eliciting priors from experts</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What is normality?</td>\n      <td>title</td>\n      <td>18</td>\n      <td>What is normality ?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What are some valuable Statistical Analysis op...</td>\n      <td>title</td>\n      <td>65</td>\n      <td>What are some valuable Statistical Analysis op...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Assessing the significance of differences in d...</td>\n      <td>title</td>\n      <td>58</td>\n      <td>Assessing the significance of differences in d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Two Cultures: statistics vs. machine learn...</td>\n      <td>title</td>\n      <td>50</td>\n      <td>The Two Cultures : statistics vs. machine lear...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit Dataset to Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles = df[df.category == 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "91648"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(df_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "809166"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCtr = Counter()\n",
    "\n",
    "def collect_chars(txt):\n",
    "    global charCtr   \n",
    "    charCtr.update(list(txt))\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(' ', 728098),\n ('e', 490819),\n ('i', 408741),\n ('t', 394552),\n ('a', 368819),\n ('o', 357424),\n ('n', 342324),\n ('r', 300101),\n ('s', 296598),\n ('l', 182752),\n ('d', 149191),\n ('c', 139820),\n ('m', 123104),\n ('u', 112104),\n ('f', 110940)]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_titles['text'].apply(collect_chars)\n",
    "charCtr.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\n\\nStrategy\\n\\n* work on text_to_instance first. \\n* when you implement read you could probably pull it from the dataframe.  In the real world this might be slow.\\n\\nQuestions\\n\\n* What are token indexers?\\n* Do I need both input and output tokens captured in my data reader?  I assume I can make the call. See example below.\\n* \\n'"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#left off here -https://jbarrow.ai/allennlp-the-hard-way-1/ (Good info on testing your datasetReader)\n",
    "\n",
    "'''\n",
    "\n",
    "Strategy\n",
    "\n",
    "* work on text_to_instance first. \n",
    "* when you implement read you could probably pull it from the dataframe.  In the real world this might be slow.\n",
    "\n",
    "Questions\n",
    "\n",
    "* What are token indexers?\n",
    "* Do I need both input and output tokens captured in my data reader?  I assume I can make the call. See example below.\n",
    "* \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b2ca36a86048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharDatasetReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_titles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/allenNLP/lib/python3.8/site-packages/allennlp/data/dataset_readers/dataset_reader.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    223\u001b[0m                     \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_worker_islice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_worker_islice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;31m# Then some validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allenNLP/lib/python3.8/site-packages/allennlp/data/dataset_readers/dataset_reader.py\u001b[0m in \u001b[0;36m_multi_worker_islice\u001b[0;34m(self, iterable, transform, ensure_lazy)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mislice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwrap_with_tqdm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mislice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allenNLP/lib/python3.8/site-packages/allennlp/common/tqdm.py\u001b[0m in \u001b[0;36mtqdm\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mnew_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"mininterval\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_mininterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/allenNLP/lib/python3.8/site-packages/tqdm/__init__.py\u001b[0m in \u001b[0;36mtqdm_notebook\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m          \u001b[0;34m\"Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m          TqdmDeprecationWarning, stacklevel=2)\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allenNLP/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         self.container = self.status_printer(\n\u001b[0m\u001b[1;32m    219\u001b[0m             self.fp, total, self.desc, self.ncols)\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/allenNLP/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Prepare IPython progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "reader = CharDatasetReader()\n",
    "reader.read(df_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register('char_lstm')\n",
    "class CharLSTM(Model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PytorchSeq2VecWrapper(\n",
    "     LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CharacterTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ct.tokenize('this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start and end symbols you should use when training your language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "\n",
    "\n",
    "tokens.insert(0, Token(START_SYMBOL))\n",
    "tokens.append(Token(END_SYMBOL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    " \n",
    "token_indexers = {'tokens': SingleIdTokenIndexer()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the beginning of how you'd create a DatasetReader.  Implement `_read` function to return an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.instance import Instance\n",
    " \n",
    "input_field = TextField(tokens[:-1], token_indexers)\n",
    "output_field = TextField(tokens[1:], token_indexers)\n",
    "instance = Instance({'input_tokens': input_field,\n",
    "                     'output_tokens': output_field})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliciting priors from experts\n"
     ]
    }
   ],
   "source": [
    "for row in df.itertuples(index=False):\n",
    "    print(row.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allenNLP",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}