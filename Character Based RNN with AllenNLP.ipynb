{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "This time we can use a larger chunk of the original dataset without running into so many memory and runtime problems.\n",
    "\n",
    "1. x - Explore and analyze the set of unique characters present in the dataset.\n",
    "1. x - Subsample the dataset to take into account the titles only.\n",
    "1. x - Implement the tokenization of the dataset and transform the tokens into AllenNLP Instances.\n",
    "1. Design a RNN with AllenNLP that includes:\n",
    "   1. an embedding of the tokens,\n",
    "   1. a seq2seq LSTM layer,\n",
    "   1. a feed forward layer that outputs probability distribution of the characters\n",
    "1. Train the model.\n",
    "1. Evaluate the model by calculating the loss of some sentences and by generating text.\n",
    "\n",
    "### Bonus\n",
    "\n",
    "Can you swap a Transformer-based model in for an LSTM? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* [AllenNLP the hard way: Building a Baseline Model](https://jbarrow.ai/allennlp-the-hard-way-2/)\n",
    "* [Sequential Labeling and Language Modeling (Chapter 5 - Covers allenNLP and Char RNN LM)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-5/v-5/35)\n",
    "  - [Related Colab Jupyter Notebook](https://colab.research.google.com/github/mhagiwara/realworldnlp/blob/master/examples/generation/lm.ipynb#scrollTo=CX2cfBPu1bfw)\n",
    "* [Atentions and the Transformer (Chapter 8)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-8/v-5/)\n",
    "* [Sequence to Sequence Models (Chapter 6)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-6/v-5/)\n",
    "* [An In Depth AllenNLP tutorial (Basics to BERT)](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)\n",
    "* [Official AllenNLP tutorial](https://allennlp.org/tutorials)\n",
    "* [AllenNLP Guide - Reading Data](https://guide.allennlp.org/reading-data)\n",
    "* [AllenNLP Simple Language Model DatasetReader](https://github.com/allenai/allennlp-models/blob/master/allennlp_models/lm/dataset_readers/simple_language_modeling.py)\n",
    "* [Testing your DatasetReader](https://jbarrow.ai/allennlp-the-hard-way-1/)\n",
    "* [AllenNLP - Vocabulary](https://guide.allennlp.org/reading-data#3)\n",
    "* [sequence_cross_entropy_loss_with_logits](https://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L704) - Looks like the simplest way to calulate the loss for your CharRNN. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK_FILE= 'stackexchange_tokenized.csv'\n",
    "file_path = f'data/{TOK_FILE}'\n",
    "df = pd.read_csv(f'data/{TOK_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "      <td>29</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "      <td>18</td>\n",
       "      <td>What is normality ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "      <td>65</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "      <td>58</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "      <td>50</td>\n",
       "      <td>The Two Cultures : statistics vs. machine lear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  post_id  parent_id  comment_id  \\\n",
       "0           0        1        NaN         NaN   \n",
       "1           1        2        NaN         NaN   \n",
       "2           2        3        NaN         NaN   \n",
       "3           3        4        NaN         NaN   \n",
       "4           4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  length  \\\n",
       "0                      Eliciting priors from experts    title      29   \n",
       "1                                 What is normality?    title      18   \n",
       "2  What are some valuable Statistical Analysis op...    title      65   \n",
       "3  Assessing the significance of differences in d...    title      58   \n",
       "4  The Two Cultures: statistics vs. machine learn...    title      50   \n",
       "\n",
       "                                           tokenized  \n",
       "0                      Eliciting priors from experts  \n",
       "1                                What is normality ?  \n",
       "2  What are some valuable Statistical Analysis op...  \n",
       "3  Assessing the significance of differences in d...  \n",
       "4  The Two Cultures : statistics vs. machine lear...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_df_size = int(1e7)\n",
    "df_size = len(df.index)\n",
    "\n",
    "if df_size > max_df_size:\n",
    "    df_size = max_df_size\n",
    "\n",
    "indxs     = np.arange(df_size)\n",
    "train_i, test_i = train_test_split(indxs, test_size=0.4, random_state=42) \n",
    "\n",
    "df_train = df.iloc[train_i]\n",
    "df_test  = df.iloc[test_i]\n",
    "# TODO: add a validation dataset later? \n",
    "\n",
    "df_train.to_csv('data/char-rnn-train.csv')\n",
    "df_test.to_csv('data/char-rnn-test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit Dataset to Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles = df[df.category == 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91648"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "809166"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCtr = Counter()\n",
    "\n",
    "def collect_chars(txt):\n",
    "    global charCtr   \n",
    "    charCtr.update(list(txt))\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 1456196),\n",
       " ('e', 981638),\n",
       " ('i', 817482),\n",
       " ('t', 789104),\n",
       " ('a', 737638),\n",
       " ('o', 714848),\n",
       " ('n', 684648),\n",
       " ('r', 600202),\n",
       " ('s', 593196),\n",
       " ('l', 365504),\n",
       " ('d', 298382),\n",
       " ('c', 279640),\n",
       " ('m', 246208),\n",
       " ('u', 224208),\n",
       " ('f', 221880)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titles['text'].apply(collect_chars)\n",
    "charCtr.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nStrategy\\n\\n* work on text_to_instance first. \\n* when you implement read you could probably pull it from the dataframe.  In the real world this might be slow.\\n\\nQuestions\\n\\n* What are token indexers?\\n* Do I need both input and output tokens captured in my data reader?  I assume I can make the call. See example below.\\n* \\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/allenai/allennlp/issues/1998\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Strategy\n",
    "\n",
    "* work on text_to_instance first. \n",
    "* when you implement read you could probably pull it from the dataframe.  In the real world this might be slow.\n",
    "\n",
    "Questions\n",
    "\n",
    "* What are token indexers?\n",
    "* Do I need both input and output tokens captured in my data reader?  I assume I can make the call. See example below.\n",
    "* \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d433db296a840bf9a2fd609fbdc5ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<allennlp.data.dataset_readers.dataset_reader.AllennlpDataset at 0x7fd622b7e700>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = CharDatasetReader()\n",
    "reader.read(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "BATCH_SIZE=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fd8b7652bd4254b4fcc690123f44b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b35ee107444d1f8bf2e618929fe427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=55110.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: try using the datasetreader to accomplish this as done at the following link\n",
    "# https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/\n",
    "\n",
    "# construct a datasetreader from 'data/char-rnn-train.csv'\n",
    "train_reader = CharDatasetReader()\n",
    "train_ds = train_reader.read('data/char-rnn-train.csv')\n",
    "# use reeader to get vocab like vocab = Vocabulary.from_instances(train_ds, max_vocab_size=XXXX)\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_ds, max_vocab_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#validation dataset / dataloader\n",
    "\n",
    "### LEFT OFF HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "embedder = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "clm = CharLanguageModel(vocab,embedder, encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct an Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is not strictly necessary but if you construct one you can pass it to the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct a Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer  = optim.Adam(clm.parameters(), lr=5.e-3)\n",
    "num_epochs = 1\n",
    "\n",
    "trainer = GradientDescentTrainer(\n",
    "    model=clm,\n",
    "    optimizer=optimizer,\n",
    "    #iterator=iterator,\n",
    "    data_loader=train_dl,\n",
    "    #valid_data_loader=valid_dl,  #TODO: add this later\n",
    "    num_epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register('char_lstm')\n",
    "class CharLSTM(Model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PytorchSeq2VecWrapper(\n",
    "     LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CharacterTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ct.tokenize('this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start and end symbols you should use when training your language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "\n",
    "\n",
    "tokens.insert(0, Token(START_SYMBOL))\n",
    "tokens.append(Token(END_SYMBOL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    " \n",
    "token_indexers = {'tokens': SingleIdTokenIndexer()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the beginning of how you'd create a DatasetReader.  Implement `_read` function to return an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.instance import Instance\n",
    " \n",
    "input_field = TextField(tokens[:-1], token_indexers)\n",
    "output_field = TextField(tokens[1:], token_indexers)\n",
    "instance = Instance({'input_tokens': input_field,\n",
    "                     'output_tokens': output_field})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliciting priors from experts\n"
     ]
    }
   ],
   "source": [
    "for row in df.itertuples(index=False):\n",
    "    print(row.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Messing with Fields / TextFields\n",
    "\n",
    "* [TextFields](https://guide.allennlp.org/representing-text-as-features#2)\n",
    "* [Field API](https://guide.allennlp.org/reading-data#1)\n",
    "* [TextFieldTensors](https://guide.allennlp.org/representing-text-as-features#9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.fields import TextField\n",
    "\n",
    "tokens = [Token('The'), Token('best'), Token('movie'), Token('ever'), Token('!')]\n",
    "token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
    "text_field = TextField(tokens, token_indexers=token_indexers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.add_tokens_to_namespace(['The', 'best', 'movie', 'ever', '!'], namespace='token_vocab')\n",
    "text_field.index(vocab)\n",
    "padding_lengths = text_field.get_padding_lengths()\n",
    "tft = text_field.as_tensor(padding_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TextFieldTensors = Dict[str, Dict[str, torch.Tensor]] #type definition\n",
    "\n",
    "from allennlp.data import TextFieldTensors\n",
    "#isinstance(tft, TextFieldTensors) #this won't work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokens'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft['tokens']['tokens'] #this looks so strange. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allenNLP",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
