{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "This time we can use a larger chunk of the original dataset without running into so many memory and runtime problems.\n",
    "\n",
    "1. x - Explore and analyze the set of unique characters present in the dataset.\n",
    "1. x - Subsample the dataset to take into account the titles only.\n",
    "1. x - Implement the tokenization of the dataset and transform the tokens into AllenNLP Instances.\n",
    "1. Design a RNN with AllenNLP that includes:\n",
    "   1. an embedding of the tokens,\n",
    "   1. a seq2seq LSTM layer,\n",
    "   1. a feed forward layer that outputs probability distribution of the characters\n",
    "1. Train the model.\n",
    "1. Evaluate the model by calculating the loss of some sentences and by generating text.\n",
    "\n",
    "### Bonus\n",
    "\n",
    "Can you swap a Transformer-based model in for an LSTM? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* [AllenNLP the hard way: Building a Baseline Model](https://jbarrow.ai/allennlp-the-hard-way-2/)\n",
    "* [Sequential Labeling and Language Modeling (Chapter 5 - Covers allenNLP and Char RNN LM)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-5/v-5/35)\n",
    "* [Atentions and the Transformer (Chapter 8)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-8/v-5/)\n",
    "* [Sequence to Sequence Models (Chapter 6)](https://livebook.manning.com/book/real-world-natural-language-processing/chapter-6/v-5/)\n",
    "* [An In Depth AllenNLP tutorial (Basics to BERT)](https://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)\n",
    "* [Official AllenNLP tutorial](https://allennlp.org/tutorials)\n",
    "* [AllenNLP Guide - Reading Data](https://guide.allennlp.org/reading-data)\n",
    "* [AllenNLP Simple Language Model DatasetReader](https://github.com/allenai/allennlp-models/blob/master/allennlp_models/lm/dataset_readers/simple_language_modeling.py)\n",
    "* [Testing your DatasetReader](https://jbarrow.ai/allennlp-the-hard-way-1/)\n",
    "* [AllenNLP - Vocabulary](https://guide.allennlp.org/reading-data#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-574725294bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/repos/language-models-manning-lp/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'char_lm_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCharLauageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m   def __init__(self,\n\u001b[1;32m     45\u001b[0m           \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/language-models-manning-lp/utils.py\u001b[0m in \u001b[0;36mCharLauageModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCharLauageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   def __init__(self,\n\u001b[0;32m---> 45\u001b[0;31m           \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m           \u001b[0membedder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTextFieldEmbedder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSeq2SeqEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#you pass in the model with layers here. LSTM, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK_FILE= 'stackexchange_tokenized.csv'\n",
    "file_path = f'data/{TOK_FILE}'\n",
    "df = pd.read_csv(f'data/{TOK_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>length</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "      <td>29</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "      <td>18</td>\n",
       "      <td>What is normality ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "      <td>65</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "      <td>58</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "      <td>50</td>\n",
       "      <td>The Two Cultures : statistics vs. machine lear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  post_id  parent_id  comment_id  \\\n",
       "0           0        1        NaN         NaN   \n",
       "1           1        2        NaN         NaN   \n",
       "2           2        3        NaN         NaN   \n",
       "3           3        4        NaN         NaN   \n",
       "4           4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  length  \\\n",
       "0                      Eliciting priors from experts    title      29   \n",
       "1                                 What is normality?    title      18   \n",
       "2  What are some valuable Statistical Analysis op...    title      65   \n",
       "3  Assessing the significance of differences in d...    title      58   \n",
       "4  The Two Cultures: statistics vs. machine learn...    title      50   \n",
       "\n",
       "                                           tokenized  \n",
       "0                      Eliciting priors from experts  \n",
       "1                                What is normality ?  \n",
       "2  What are some valuable Statistical Analysis op...  \n",
       "3  Assessing the significance of differences in d...  \n",
       "4  The Two Cultures : statistics vs. machine lear...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit Dataset to Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles = df[df.category == 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91648"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "809166"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "charCtr = Counter()\n",
    "\n",
    "def collect_chars(txt):\n",
    "    global charCtr   \n",
    "    charCtr.update(list(txt))\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 728098),\n",
       " ('e', 490819),\n",
       " ('i', 408741),\n",
       " ('t', 394552),\n",
       " ('a', 368819),\n",
       " ('o', 357424),\n",
       " ('n', 342324),\n",
       " ('r', 300101),\n",
       " ('s', 296598),\n",
       " ('l', 182752),\n",
       " ('d', 149191),\n",
       " ('c', 139820),\n",
       " ('m', 123104),\n",
       " ('u', 112104),\n",
       " ('f', 110940)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titles['text'].apply(collect_chars)\n",
    "charCtr.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nStrategy\\n\\n* work on text_to_instance first. \\n* when you implement read you could probably pull it from the dataframe.  In the real world this might be slow.\\n\\nQuestions\\n\\n* What are token indexers?\\n* Do I need both input and output tokens captured in my data reader?  I assume I can make the call. See example below.\\n* \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/allenai/allennlp/issues/1998\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Strategy\n",
    "\n",
    "* work on text_to_instance first. \n",
    "* when you implement read you could probably pull it from the dataframe.  In the real world this might be slow.\n",
    "\n",
    "Questions\n",
    "\n",
    "* What are token indexers?\n",
    "* Do I need both input and output tokens captured in my data reader?  I assume I can make the call. See example below.\n",
    "* \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c074744e55064e878c33c7a7fc5688d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<allennlp.data.dataset_readers.dataset_reader.AllennlpDataset at 0x7f66b4756af0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = CharDatasetReader()\n",
    "reader.read(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Model.register('char_lstm')\n",
    "class CharLSTM(Model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PytorchSeq2VecWrapper(\n",
    "     LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CharacterTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ct.tokenize('this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start and end symbols you should use when training your language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "\n",
    "\n",
    "tokens.insert(0, Token(START_SYMBOL))\n",
    "tokens.append(Token(END_SYMBOL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    " \n",
    "token_indexers = {'tokens': SingleIdTokenIndexer()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the beginning of how you'd create a DatasetReader.  Implement `_read` function to return an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.instance import Instance\n",
    " \n",
    "input_field = TextField(tokens[:-1], token_indexers)\n",
    "output_field = TextField(tokens[1:], token_indexers)\n",
    "instance = Instance({'input_tokens': input_field,\n",
    "                     'output_tokens': output_field})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliciting priors from experts\n"
     ]
    }
   ],
   "source": [
    "for row in df.itertuples(index=False):\n",
    "    print(row.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Messing with Fields / TextFields\n",
    "\n",
    "* [TextFields](https://guide.allennlp.org/representing-text-as-features#2)\n",
    "* [Field API](https://guide.allennlp.org/reading-data#1)\n",
    "* [TextFieldTensors](https://guide.allennlp.org/representing-text-as-features#9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.fields import TextField\n",
    "\n",
    "tokens = [Token('The'), Token('best'), Token('movie'), Token('ever'), Token('!')]\n",
    "token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
    "text_field = TextField(tokens, token_indexers=token_indexers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.add_tokens_to_namespace(['The', 'best', 'movie', 'ever', '!'], namespace='token_vocab')\n",
    "text_field.index(vocab)\n",
    "padding_lengths = text_field.get_padding_lengths()\n",
    "tft = text_field.as_tensor(padding_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TextFieldTensors = Dict[str, Dict[str, torch.Tensor]] #type definition\n",
    "#isinstance(tft, TextFieldTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokens'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft['tokens']['tokens'] #this looks so strange. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allenNLP",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
